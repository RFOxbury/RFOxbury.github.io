<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Rosie Oxbury" />

<meta name="date" content="2019-10-08" />

<title>ICPhS: regression analyses</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Rosie's website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="papers_and_presentations.html">Papers and presentations</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Scrapbook
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="icphs_plotting.html">ICPhS: vowel plot</a>
    </li>
    <li>
      <a href="normalisation_oct25.html">Normalisation</a>
    </li>
    <li>
      <a href="vowel_quad.html">Representing vowel changes</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">ICPhS: regression analyses</h1>
<h4 class="author">Rosie Oxbury</h4>
<h4 class="date">10/08/2019</h4>

</div>


<p>Below is the code (a) for the statistical analysis Oxbury &amp; McCarthy did for their ICPhS 2019 poster, and (b) for the violin plots that appeared at the bottom of the poster.</p>
<p><img src="icphs_poster.png" style="width:50%; margin-right: 20px" align="centre"></p>
<div id="cleaning" class="section level1">
<h1>Cleaning</h1>
<p>First, load necessary packages:</p>
<pre class="r"><code>library(tidyverse)
library(brms)</code></pre>
<pre><code>## Loading required package: Rcpp</code></pre>
<pre><code>## Loading &#39;brms&#39; package (version 2.9.0). Useful instructions
## can be found by typing help(&#39;brms&#39;). A more detailed introduction
## to the package is available through vignette(&#39;brms_overview&#39;).</code></pre>
<pre class="r"><code>library(sjstats)
library(bayesplot)</code></pre>
<pre><code>## This is bayesplot version 1.7.0</code></pre>
<pre><code>## - Online documentation and vignettes at mc-stan.org/bayesplot</code></pre>
<pre><code>## - bayesplot theme set to bayesplot::theme_default()</code></pre>
<pre><code>##    * Does _not_ affect other ggplot2 plots</code></pre>
<pre><code>##    * See ?bayesplot_theme_set for details on theme setting</code></pre>
<p>Load in the data:</p>
<pre class="r"><code>diphs &lt;- read_csv(&quot;icphs_data.csv&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   participant = col_character(),
##   sound_label = col_character(),
##   word = col_character(),
##   task = col_character(),
##   age = col_character(),
##   gender = col_character(),
##   rol.var = col_character(),
##   face.l = col_character(),
##   price.l = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<p>Alter duration so that (a) there are no tokens with duration longer than 0.75 seconds, and (b) it is log-transformed.</p>
<pre class="r"><code>ggplot(diphs, aes(x = duration, fill = age)) + geom_histogram(bins = 100)</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /> Filter so that duration has to be less than 0.75, then see what it looks like.</p>
<pre class="r"><code>diphs2 &lt;- diphs %&gt;% filter(duration &lt; 0.75)</code></pre>
<p>See what it looks like log-transformed:</p>
<pre class="r"><code>ggplot(diphs2, aes(x = log10(duration), fill = age)) + geom_histogram(bins = 100)</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>diphs3 &lt;- mutate(diphs2,
               LogDur = log10(duration))</code></pre>
<p>… and standardize it</p>
<pre class="r"><code>diphs3 &lt;- mutate(diphs3,
               Log_dur_z = scale(LogDur))</code></pre>
<p>Check that all the other variables are OK. Convert some of them to factors</p>
<pre class="r"><code>str(diphs3)</code></pre>
<pre><code>## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 8548 obs. of  60 variables:
##  $ participant : chr  &quot;Amanda&quot; &quot;Amanda&quot; &quot;Amanda&quot; &quot;Amanda&quot; ...
##  $ sound_label : chr  &quot;face&quot; &quot;goat&quot; &quot;price&quot; &quot;price&quot; ...
##  $ sound_start : num  7.26 44.6 47.14 91.27 95.22 ...
##  $ sound_end   : num  7.6 44.7 47.2 91.3 95.3 ...
##  $ word        : chr  &quot;HEY&quot; &quot;GO&quot; &quot;FIND&quot; &quot;INSIDE&quot; ...
##  $ F1_20       : num  840 502 847 657 598 ...
##  $ F1_35       : num  717 533 878 759 637 ...
##  $ F1_50       : num  633 590 863 799 602 ...
##  $ F1_65       : num  642 527 834 802 580 ...
##  $ F1_80       : num  604 470 808 724 573 ...
##  $ F2_20       : num  1875 1744 1528 1507 1163 ...
##  $ F2_35       : num  1904 1831 1642 1475 1076 ...
##  $ F2_50       : num  2129 1867 1692 1415 1144 ...
##  $ F2_65       : num  2094 1888 1790 1456 948 ...
##  $ F2_80       : num  2119 1842 1868 1501 1239 ...
##  $ task        : chr  &quot;soc.int&quot; &quot;soc.int&quot; &quot;soc.int&quot; &quot;soc.int&quot; ...
##  $ age         : chr  &quot;adolescent&quot; &quot;adolescent&quot; &quot;adolescent&quot; &quot;adolescent&quot; ...
##  $ gender      : chr  &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ...
##  $ meanF1      : num  687 524 846 748 598 ...
##  $ meanF2      : num  2024 1834 1704 1471 1114 ...
##  $ meanFleeceF1: num  460 460 460 460 460 ...
##  $ meanFleeceF2: num  2648 2648 2648 2648 2648 ...
##  $ meanTRAPF1  : num  853 853 853 853 853 ...
##  $ u_F1        : num  460 460 460 460 460 ...
##  $ u_F2        : num  460 460 460 460 460 ...
##  $ trapF2      : num  1554 1554 1554 1554 1554 ...
##  $ S_F1        : num  591 591 591 591 591 ...
##  $ S_F2        : num  1554 1554 1554 1554 1554 ...
##  $ normF1_20   : num  1.421 0.849 1.433 1.112 1.011 ...
##  $ normF1_35   : num  1.213 0.901 1.485 1.285 1.078 ...
##  $ normF1_50   : num  1.072 0.998 1.461 1.351 1.019 ...
##  $ normF1_65   : num  1.087 0.892 1.411 1.357 0.982 ...
##  $ normF1_80   : num  1.022 0.796 1.368 1.225 0.969 ...
##  $ normF2_20   : num  1.207 1.122 0.984 0.97 0.748 ...
##  $ normF2_35   : num  1.225 1.178 1.057 0.949 0.692 ...
##  $ normF2_50   : num  1.37 1.201 1.089 0.911 0.736 ...
##  $ normF2_65   : num  1.348 1.215 1.152 0.937 0.61 ...
##  $ normF2_80   : num  1.363 1.186 1.202 0.966 0.797 ...
##  $ duration    : num  0.3321 0.0872 0.0859 0.0715 0.0792 ...
##  $ changeF1    : num  -236 -31.2 -38.6 67.3 -25 ...
##  $ normChangeF1: num  -0.3994 -0.0528 -0.0654 0.1139 -0.0423 ...
##  $ changeF2    : num  243.41 98.4 339.78 -5.69 75.84 ...
##  $ normChangeF2: num  0.15664 0.06332 0.21865 -0.00366 0.0488 ...
##  $ VL          : num  339 103.2 342 67.6 79.9 ...
##  $ normVL      : num  0.429 0.0825 0.2282 0.114 0.0646 ...
##  $ normVSL1    : num  0.2087 0.0766 0.0896 0.1745 0.0869 ...
##  $ normVSL2    : num  0.2024 0.0996 0.04 0.0767 0.073 ...
##  $ normVSL3    : num  0.0269 0.1069 0.0808 0.0265 0.1323 ...
##  $ normVSL4    : num  0.067 0.1006 0.0661 0.1347 0.1877 ...
##  $ VSL1        : num  126.2 92.2 117.7 107.3 95.2 ...
##  $ VSL2        : num  239.5 67.6 51.7 71.6 76.5 ...
##  $ VSL3        : num  35.4 66.3 102.7 40.4 198.2 ...
##  $ VSL4        : num  45.5 73.1 82 90.2 291.1 ...
##  $ TrajLength  : num  447 299 354 310 661 ...
##  $ norm_TL     : num  0.505 0.384 0.276 0.413 0.48 ...
##  $ rol.var     : chr  &quot;n.a.&quot; &quot;no.rolling&quot; &quot;n.a.&quot; &quot;n.a.&quot; ...
##  $ face.l      : chr  &quot;fine&quot; &quot;n.a.&quot; &quot;n.a.&quot; &quot;n.a.&quot; ...
##  $ price.l     : chr  &quot;n.a.&quot; &quot;n.a.&quot; &quot;fine&quot; &quot;fine&quot; ...
##  $ LogDur      : num  -0.479 -1.059 -1.066 -1.146 -1.101 ...
##  $ Log_dur_z   : num [1:8548, 1] 2.185 -0.685 -0.718 -1.112 -0.893 ...
##   ..- attr(*, &quot;scaled:center&quot;)= num -0.921
##   ..- attr(*, &quot;scaled:scale&quot;)= num 0.202</code></pre>
<p>Age as factor and sex as factor</p>
<pre class="r"><code>diphs3$age &lt;- as.factor(diphs3$age)
diphs3$gender &lt;- as.factor(diphs3$gender)</code></pre>
<p>Participant and word as factors too</p>
<pre class="r"><code>diphs3$participant &lt;- as.factor(diphs3$participant)
diphs3$word &lt;- as.factor(diphs3$word)</code></pre>
<p>Change the gender variable to sex</p>
<pre class="r"><code>diphs3 &lt;- diphs3 %&gt;% rename(Sex = gender)</code></pre>
<p>Finally, we are going to recode that variable so that male becomes the reference level in the models we build.</p>
<pre class="r"><code>diphs3$Sex &lt;- relevel(diphs3$Sex, ref = &quot;M&quot;)</code></pre>
<p>Now onto some modelling.</p>
</div>
<div id="face" class="section level1">
<h1>FACE</h1>
<p>Make the subset:</p>
<pre class="r"><code>face &lt;- diphs3 %&gt;% filter(sound_label == &quot;face&quot;)</code></pre>
<div id="face-f1" class="section level2">
<h2>FACE F1</h2>
<p>N.B. in the real analysis, I checked the distribution of the dependent and independent variables, looked for outliers and so on… I am not putting all of that here because it would make the page pretty long :)</p>
<p>OK, now we should probably set some priors.</p>
<p>Check distribution of each of these to know what is a normal range:</p>
<pre class="r"><code>ggplot(face, aes(x=Log_dur_z)) + geom_histogram(bins = 100)</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>ggplot(face, aes(x=age, y = normF1_20, fill = Sex)) + geom_violin()</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-14-2.png" width="672" /></p>
<p>Find out what priors we need:</p>
<pre class="r"><code>get_prior(normF1_20 ~ Log_dur_z + age*Sex + (1|participant) + (1|word), data = face)</code></pre>
<pre><code>##                  prior     class          coef       group resp dpar nlpar
## 1                              b                                          
## 2                              b      agechild                            
## 3                              b agechild:SexF                            
## 4                              b     Log_dur_z                            
## 5                              b          SexF                            
## 6  student_t(3, 1, 10) Intercept                                          
## 7  student_t(3, 0, 10)        sd                                          
## 8                             sd               participant                
## 9                             sd     Intercept participant                
## 10                            sd                      word                
## 11                            sd     Intercept        word                
## 12 student_t(3, 0, 10)     sigma                                          
##    bound
## 1       
## 2       
## 3       
## 4       
## 5       
## 6       
## 7       
## 8       
## 9       
## 10      
## 11      
## 12</code></pre>
<p>Create the priors:</p>
<pre class="r"><code>priors.1 &lt;- c(set_prior(&quot;normal(1, 0.75)&quot;, class = &quot;Intercept&quot;),
              set_prior(&quot;normal(0, 0.1)&quot;, class = &quot;b&quot;, coef = &quot;Log_dur_z&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;b&quot;, coef = &quot;agechild&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;b&quot;, coef = &quot;SexF&quot;),
              set_prior(&quot;normal(0, 1)&quot;, class = &quot;b&quot;, coef = &quot;agechild:SexF&quot;), #2*age effect to allow for complete reversal
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group=&quot;participant&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group=&quot;word&quot;)
              )</code></pre>
<p>And make the model:</p>
<pre class="r"><code>model1.1 &lt;- brm(normF1_20 ~ Log_dur_z + age*Sex + (1|participant) + (1|word), data = face, prior = priors.1)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;a0e0ae155a974d155793524f8a298c1a&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000785 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 7.85 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 30.9482 seconds (Warm-up)
## Chain 1:                14.7652 seconds (Sampling)
## Chain 1:                45.7134 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;a0e0ae155a974d155793524f8a298c1a&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.000416 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 4.16 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 39.9084 seconds (Warm-up)
## Chain 2:                7.005 seconds (Sampling)
## Chain 2:                46.9134 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;a0e0ae155a974d155793524f8a298c1a&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0.000179 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.79 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 38.8427 seconds (Warm-up)
## Chain 3:                8.26133 seconds (Sampling)
## Chain 3:                47.104 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;a0e0ae155a974d155793524f8a298c1a&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0.000179 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.79 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 28.8915 seconds (Warm-up)
## Chain 4:                10.7716 seconds (Sampling)
## Chain 4:                39.6631 seconds (Total)
## Chain 4:</code></pre>
<p>Have a look at the model summary:</p>
<pre class="r"><code>summary(model1.1)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: normF1_20 ~ Log_dur_z + age * Sex + (1 | participant) + (1 | word) 
##    Data: face (Number of observations: 1935) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~participant (Number of levels: 28) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.06      0.01     0.04     0.09        967 1.00
## 
## ~word (Number of levels: 246) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.04      0.00     0.03     0.05       1301 1.00
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept         1.02      0.02     0.97     1.07        618 1.00
## Log_dur_z         0.04      0.00     0.03     0.04       6071 1.00
## agechild         -0.06      0.03    -0.13     0.01        657 1.00
## SexF             -0.00      0.03    -0.07     0.07        665 1.00
## agechild:SexF    -0.01      0.05    -0.11     0.09        689 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     0.12      0.00     0.11     0.12       4607 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>This summary indicates that:</p>
<ul>
<li><p>A 1 Z-score increase in duration predicts a 0.04 increase in F1 at 20%. 95% of the posterior distribution is between 0.03 and 0.04 for this coefficient.</p></li>
<li><p>Although the coefficient for the age variable is larger in absolute terms than that for duration, the posterior distribution is flatter, and the 95% credible interval includes 0.</p></li>
<li><p>The posterior on the coefficient for Sex is centred at 0! So there really seems to be no effect of Sex. Similarly, the coefficient for the age-sex interaction is not very impressive.</p></li>
</ul>
<p>A different kind of output:</p>
<pre class="r"><code>tidy_stan(model1.1, prob=0.89, type=&quot;all&quot;, digits=4)</code></pre>
<pre><code>## Warning in stddev/sqrt(ess): longer object length is not a multiple of
## shorter object length</code></pre>
<pre><code>## 
## # Summary Statistics of Stan-Model
## 
## ## Fixed effects:
## 
##                estimate std.error          HDI(89%)  ratio   rhat   mcse
##  Intercept       1.0182    0.0242 [ 0.9775  1.0571] 0.1545 1.0039 0.0010
##  Log_dur_z       0.0383    0.0034 [ 0.0328  0.0438] 1.5178 0.9993 0.0000
##  Log_dur_z       0.0383    0.0034 [ 0.0328  0.0438] 1.5178 0.9993 0.0000
##  agechild       -0.0563    0.0329 [-0.1108  0.0001] 0.1642 1.0045 0.0014
##  SexF           -0.0011    0.0321 [-0.0538  0.0533] 0.1662 1.0039 0.0013
##  agechild.SexF  -0.0129    0.0479 [-0.0883  0.0672] 0.1724 1.0040 0.0019
## 
## ## Random effect (Intercept: participant)
## 
##                        estimate std.error          HDI(89%)  ratio   rhat
##  participant.Amanda      0.0394    0.0265 [-0.0031  0.0830] 0.2319 1.0027
##  participant.CB         -0.0228    0.0268 [-0.0650  0.0210] 0.1864 1.0028
##  participant.Chantelle  -0.0776    0.0262 [-0.1176 -0.0354] 0.1982 1.0026
##  participant.ChrisB     -0.0764    0.0252 [-0.1176 -0.0330] 0.2126 1.0026
##  participant.F1          0.0449    0.0288 [-0.0012  0.0922] 0.3480 1.0011
##  participant.F10        -0.0233    0.0254 [-0.0669  0.0164] 0.2864 1.0010
##  participant.F3         -0.0736    0.0261 [-0.1170 -0.0320] 0.3060 1.0003
##  participant.F4          0.0854    0.0304 [ 0.0379  0.1308] 0.3640 1.0007
##  participant.F7          0.0602    0.0278 [ 0.0151  0.1069] 0.3743 1.0003
##  participant.F8         -0.0979    0.0270 [-0.1427 -0.0567] 0.3233 1.0004
##  participant.F9          0.0065    0.0266 [-0.0339  0.0526] 0.3253 1.0005
##  participant.Ibrahim    -0.0323    0.0261 [-0.0730  0.0133] 0.1924 1.0030
##  participant.Jessica     0.0583    0.0270 [ 0.0154  0.1012] 0.2243 1.0029
##  participant.Lola       -0.0169    0.0252 [-0.0577  0.0238] 0.2066 1.0026
##  participant.Lucy        0.0703    0.0271 [ 0.0299  0.1163] 0.2431 1.0020
##  participant.M1          0.0572    0.0281 [ 0.0103  0.0995] 0.3381 1.0041
##  participant.M3         -0.0265    0.0283 [-0.0703  0.0199] 0.3916 1.0044
##  participant.M4          0.0716    0.0251 [ 0.0294  0.1125] 0.3014 1.0059
##  participant.M5         -0.0371    0.0268 [-0.0823  0.0037] 0.2998 1.0052
##  participant.M6         -0.0962    0.0277 [-0.1404 -0.0518] 0.3352 1.0035
##  participant.M7          0.0100    0.0280 [-0.0350  0.0559] 0.3667 1.0041
##  participant.M8          0.0133    0.0269 [-0.0328  0.0556] 0.2930 1.0043
##  participant.Matisse     0.0205    0.0265 [-0.0217  0.0638] 0.1837 1.0029
##  participant.Omar       -0.0143    0.0271 [-0.0568  0.0305] 0.1854 1.0044
##  participant.Sami        0.0421    0.0257 [-0.0018  0.0837] 0.1929 1.0028
##  participant.Shantel     0.0023    0.0269 [-0.0418  0.0449] 0.2217 1.0034
##  participant.Tariq       0.0310    0.0265 [-0.0106  0.0762] 0.1864 1.0035
##  participant.ZR         -0.0266    0.0266 [-0.0709  0.0168] 0.1866 1.0035
##   mcse
##  1e-03
##  1e-03
##  9e-04
##  7e-04
##  9e-04
##  8e-04
##  7e-04
##  8e-04
##  8e-04
##  8e-04
##  1e-03
##  9e-04
##  9e-04
##  8e-04
##  7e-04
##  7e-04
##  8e-04
##  8e-04
##  7e-04
##  7e-04
##  8e-04
##  1e-03
##  1e-03
##  1e-03
##  9e-04
##  1e-03
##  1e-03
##  4e-04
## 
## ## Random effect (Intercept: word...APRIL..)
## 
##                 estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word...APRIL..   -0.015    0.0411 [-0.0803  0.0523] 1.2777 1.0002 5e-04
## 
## ## Random effect (Intercept: word...EIGHTEENTH..)
## 
##                      estimate std.error          HDI(89%)  ratio   rhat
##  word...EIGHTEENTH..  -0.0019    0.0399 [-0.0662  0.0593] 1.7284 0.9995
##   mcse
##  6e-04
## 
## ## Random effect (Intercept: word...FACE..)
## 
##                estimate std.error          HDI(89%)  ratio rhat  mcse
##  word...FACE..  -0.0399    0.0398 [-0.1017  0.0250] 1.1602    1 6e-04
## 
## ## Random effect (Intercept: word)
## 
##                     estimate std.error          HDI(89%)  ratio   rhat
##  word.A               0.0071    0.0177 [-0.0201  0.0360] 1.2213 1.0010
##  word.ABBREVIATIONS  -0.0137    0.0396 [-0.0720  0.0532] 1.4710 0.9992
##  word.ABLE           -0.0113    0.0395 [-0.0766  0.0460] 1.4751 1.0004
##  word.ACE            -0.0010    0.0404 [-0.0644  0.0637] 1.3441 0.9994
##  word.ACHE           -0.0205    0.0393 [-0.0856  0.0391] 1.4815 0.9993
##  word.ACQUAINTANCES  -0.0067    0.0398 [-0.0712  0.0577] 1.4281 0.9993
##  word.AEROPLANE      -0.0004    0.0368 [-0.0626  0.0595] 1.6118 1.0003
##  word.AGE            -0.0791    0.0282 [-0.1225 -0.0334] 0.9654 1.0002
##  word.ALWAYS         -0.0277    0.0361 [-0.0864  0.0307] 1.7737 1.0004
##  word.AMAZINGLY      -0.0092    0.0405 [-0.0750  0.0539] 1.4091 0.9996
##  word.ANYWAY         -0.0024    0.0358 [-0.0609  0.0548] 1.4473 0.9996
##  word.ANYWAYS        -0.0200    0.0402 [-0.0826  0.0434] 1.4544 0.9993
##  word.APRIL           0.0051    0.0324 [-0.0449  0.0579] 1.6343 0.9997
##  word.ASIAN          -0.0021    0.0374 [-0.0622  0.0629] 1.5074 0.9998
##  word.ATE             0.0029    0.0381 [-0.0566  0.0619] 1.4247 0.9995
##  word.AWAY            0.0085    0.0231 [-0.0279  0.0450] 1.2180 1.0006
##  word.BABIES         -0.0012    0.0385 [-0.0672  0.0585] 1.3747 0.9997
##  word.BABY           -0.0431    0.0154 [-0.0674 -0.0184] 1.3138 0.9993
##  word.BAIT            0.0029    0.0370 [-0.0596  0.0610] 1.6292 1.0002
##  word.BASED           0.0097    0.0387 [-0.0510  0.0713] 1.6646 0.9992
##  word.BASICALLY      -0.0186    0.0176 [-0.0467  0.0112] 1.1435 1.0001
##  word.BECAME         -0.0072    0.0381 [-0.0640  0.0569] 1.6302 0.9997
##  word.BEHAVIOR        0.0021    0.0393 [-0.0565  0.0688] 1.6276 0.9997
##  word.BEHAVIOUR      -0.0220    0.0379 [-0.0828  0.0392] 1.6696 0.9993
##  word.BIRTHDAY       -0.0280    0.0291 [-0.0761  0.0190] 1.4349 0.9996
##  word.BLAME           0.0044    0.0401 [-0.0598  0.0699] 1.7519 0.9999
##  word.BRAVE           0.0179    0.0404 [-0.0460  0.0814] 1.4127 1.0005
##  word.BREAK           0.0117    0.0348 [-0.0444  0.0686] 1.5861 0.9999
##  word.BREAKING       -0.0063    0.0410 [-0.0709  0.0561] 1.7020 0.9993
##  word.CAKE            0.0760    0.0174 [ 0.0480  0.1030] 1.2969 0.9995
##  word.CAME           -0.0242    0.0182 [-0.0539  0.0032] 1.4058 0.9998
##  word.CAPABLE        -0.0008    0.0383 [-0.0713  0.0568] 1.4599 0.9999
##  word.CASE           -0.0147    0.0409 [-0.0828  0.0478] 1.6514 0.9998
##  word.CELEBRATE       0.0022    0.0391 [-0.0577  0.0640] 1.6815 0.9997
##  word.CELEBRATING     0.0110    0.0380 [-0.0520  0.0733] 1.4318 0.9998
##  word.CHANGE          0.0050    0.0238 [-0.0343  0.0427] 1.6283 0.9994
##  word.CHANGED         0.0008    0.0371 [-0.0645  0.0547] 1.7218 0.9995
##  word.CHANGES         0.0008    0.0413 [-0.0710  0.0599] 1.7892 0.9992
##  word.CHASE          -0.0027    0.0367 [-0.0610  0.0554] 1.3271 0.9994
##  word.CHASED         -0.0137    0.0374 [-0.0725  0.0476] 1.6257 0.9995
##  word.CHASING        -0.0062    0.0405 [-0.0689  0.0619] 1.4807 0.9996
##  word.CLAIM          -0.0024    0.0400 [-0.0719  0.0583] 1.5866 0.9995
##  word.CLAIMED         0.0353    0.0369 [-0.0209  0.1010] 1.5995 1.0004
##  word.CLAIMING        0.0175    0.0386 [-0.0455  0.0784] 1.9098 0.9996
##  word.COMMUNICATE    -0.0015    0.0394 [-0.0630  0.0627] 1.3418 1.0004
##  word.COMMUNICATION  -0.0059    0.0404 [-0.0687  0.0580] 1.4382 1.0004
##  word.CONVERSATION   -0.0256    0.0362 [-0.0851  0.0362] 1.8479 0.9992
##  word.CONVERSATIONS  -0.0152    0.0406 [-0.0832  0.0486] 1.5028 0.9993
##  word.CRAZY           0.0689    0.0287 [ 0.0242  0.1129] 1.2019 0.9993
##  word.CREATE          0.0149    0.0399 [-0.0534  0.0755] 1.6091 0.9994
##  word.CREATED        -0.0190    0.0388 [-0.0837  0.0402] 1.9714 0.9996
##  word.CREATING       -0.0062    0.0384 [-0.0636  0.0635] 1.4880 0.9992
##  word.CUPCAKE        -0.0285    0.0385 [-0.0894  0.0338] 1.3781 0.9994
##  word.DAISY          -0.0292    0.0356 [-0.0883  0.0257] 1.5216 0.9992
##  word.DATE           -0.0086    0.0355 [-0.0657  0.0485] 1.3984 0.9998
##  word.DATES          -0.0003    0.0369 [-0.0591  0.0575] 1.4385 1.0012
##  word.DAY            -0.0095    0.0200 [-0.0413  0.0197] 1.3714 0.9997
##  word.DAYLIGHT        0.0183    0.0398 [-0.0459  0.0796] 1.3502 0.9994
##  word.DAYS           -0.0637    0.0209 [-0.0978 -0.0318] 1.3773 1.0003
##  word.DISABLED       -0.0130    0.0363 [-0.0763  0.0441] 1.3774 0.9996
##  word.DONATED         0.0005    0.0374 [-0.0585  0.0658] 1.3381 1.0002
##  word.EIGHT           0.0703    0.0189 [ 0.0416  0.1011] 1.2118 0.9997
##  word.EIGHTH         -0.0281    0.0380 [-0.0885  0.0341] 1.0720 1.0001
##  word.EIGHTY         -0.0104    0.0389 [-0.0749  0.0494] 1.3472 0.9992
##  word.ELABORATE       0.0024    0.0412 [-0.0591  0.0717] 1.3834 1.0002
##  word.ELABORATED     -0.0044    0.0403 [-0.0705  0.0569] 1.6820 0.9993
##  word.ESCALATES      -0.0196    0.0379 [-0.0833  0.0374] 1.3260 0.9999
##  word.ESCAPED        -0.0730    0.0268 [-0.1152 -0.0281] 1.3556 0.9991
##  word.ESTATE         -0.0084    0.0371 [-0.0721  0.0467] 1.8345 1.0003
##  word.EXPLAIN         0.0031    0.0352 [-0.0515  0.0643] 1.6095 0.9997
##  word.FACE            0.0042    0.0237 [-0.0315  0.0399] 1.5484 0.9997
##  word.FACED          -0.0047    0.0398 [-0.0684  0.0581] 1.4939 0.9996
##  word.FACES          -0.0024    0.0383 [-0.0685  0.0605] 1.4627 0.9995
##  word.FAKE            0.0156    0.0327 [-0.0356  0.0725] 1.6558 0.9994
##  word.FAMOUS          0.0151    0.0409 [-0.0503  0.0793] 1.7491 0.9997
##  word.FAVORITE       -0.0075    0.0385 [-0.0683  0.0596] 1.4520 0.9997
##  word.FAVOURITE       0.0027    0.0389 [-0.0586  0.0641] 1.4415 1.0002
##  word.FLAMES         -0.0144    0.0376 [-0.0785  0.0458] 1.3731 0.9994
##  word.FLIRTATIOUS     0.0071    0.0379 [-0.0622  0.0669] 1.5231 0.9996
##  word.FRIDAY         -0.0117    0.0383 [-0.0738  0.0550] 1.3917 0.9999
##  word.GAME           -0.0334    0.0258 [-0.0769  0.0077] 1.5060 0.9998
##  word.GAMES          -0.0182    0.0372 [-0.0779  0.0408] 1.6626 0.9995
##  word.GATE            0.0023    0.0202 [-0.0323  0.0356] 1.4543 0.9992
##  word.GAVE           -0.0796    0.0234 [-0.1160 -0.0410] 1.2092 0.9998
##  word.GAY            -0.0151    0.0271 [-0.0582  0.0282] 1.2648 0.9997
##  word.GAYS           -0.0061    0.0396 [-0.0727  0.0543] 1.5088 0.9991
##  word.GENERATION      0.0104    0.0377 [-0.0502  0.0672] 1.5700 0.9998
##  word.GREAT           0.0277    0.0313 [-0.0279  0.0747] 1.4426 1.0000
##  word.GRENADA         0.0337    0.0389 [-0.0253  0.1005] 1.3173 0.9995
##  word.GREY            0.0330    0.0261 [-0.0118  0.0723] 1.3506 1.0001
##  word.H              -0.0165    0.0369 [-0.0714  0.0471] 1.6344 0.9994
##  word.HAIRSPRAY       0.0150    0.0388 [-0.0436  0.0828] 1.4371 0.9997
##  word.HATE           -0.0304    0.0351 [-0.0876  0.0259] 1.4050 1.0004
##  word.HATED          -0.0321    0.0321 [-0.0798  0.0207] 1.4207 0.9996
##  word.HAY             0.1098    0.0232 [ 0.0727  0.1460] 1.2058 0.9997
##  word.HAYS           -0.0001    0.0393 [-0.0659  0.0628] 1.3792 0.9998
##  word.HEY             0.0553    0.0361 [-0.0010  0.1113] 1.2982 0.9996
##  word.HEYA            0.0372    0.0394 [-0.0289  0.0991] 1.3110 0.9997
##  word.HOLIDAY         0.0186    0.0383 [-0.0474  0.0771] 1.2748 0.9997
##  word.HOLIDAYS       -0.0061    0.0377 [-0.0694  0.0499] 1.5034 0.9995
##  word.INDICATE        0.0044    0.0407 [-0.0577  0.0687] 1.5609 0.9991
##  word.INTEGRATED     -0.0098    0.0400 [-0.0776  0.0502] 1.2298 0.9997
##  word.INVADING       -0.0031    0.0392 [-0.0696  0.0577] 1.4138 0.9992
##  word.ISOLATION      -0.0169    0.0397 [-0.0821  0.0468] 1.3723 0.9991
##  word.ISRAELI         0.0002    0.0405 [-0.0682  0.0622] 1.5332 1.0006
##  word.J              -0.0148    0.0345 [-0.0699  0.0407] 1.2394 0.9994
##  word.JAMAICAN        0.0165    0.0373 [-0.0435  0.0754] 1.5761 0.9996
##  word.K              -0.0112    0.0381 [-0.0700  0.0496] 1.5449 0.9994
##  word.KAY            -0.0085    0.0274 [-0.0548  0.0346] 1.5251 1.0005
##  word.LADIES          0.0093    0.0406 [-0.0551  0.0783] 1.6236 0.9995
##  word.LADY            0.0081    0.0259 [-0.0307  0.0501] 1.7915 0.9996
##  word.LATE            0.0465    0.0333 [-0.0072  0.1002] 1.3229 1.0001
##  word.LATER           0.0065    0.0240 [-0.0339  0.0456] 1.3868 0.9994
##  word.LAYING          0.0019    0.0380 [-0.0575  0.0707] 1.4269 0.9996
##  word.MADE            0.0065    0.0229 [-0.0312  0.0450] 1.3931 1.0001
##  word.MAIN            0.0060    0.0408 [-0.0529  0.0720] 1.1009 0.9997
##  word.MAINLY          0.0191    0.0321 [-0.0333  0.0719] 1.5431 0.9999
##  word.MAINSTREAM     -0.0019    0.0400 [-0.0654  0.0648] 1.3609 0.9994
##  word.MAISONETTE      0.0087    0.0404 [-0.0529  0.0773] 1.5828 1.0007
##  word.MAJOR          -0.0140    0.0389 [-0.0756  0.0446] 1.3916 0.9992
##  word.MAJORLY        -0.0151    0.0375 [-0.0765  0.0431] 1.6944 0.9997
##  word.MAKE            0.0149    0.0198 [-0.0169  0.0454] 1.5211 0.9994
##  word.MAKES          -0.0175    0.0311 [-0.0691  0.0334] 1.5334 0.9992
##  word.MAKING          0.0237    0.0309 [-0.0266  0.0694] 1.5038 0.9994
##  word.MATES           0.0040    0.0406 [-0.0607  0.0650] 1.3677 1.0002
##  word.MAY             0.0059    0.0395 [-0.0578  0.0696] 1.4378 1.0000
##  word.MAYBE          -0.0034    0.0258 [-0.0445  0.0365] 1.4031 0.9997
##  word.MEDICATION     -0.0149    0.0407 [-0.0759  0.0551] 1.4932 0.9995
##  word.MISTAKE        -0.0302    0.0395 [-0.0952  0.0294] 1.4366 0.9996
##  word.MISTAKES       -0.0234    0.0363 [-0.0792  0.0371] 1.4547 0.9995
##  word.MKAY            0.0109    0.0372 [-0.0446  0.0701] 1.7151 0.9994
##  word.MONDAY          0.0139    0.0348 [-0.0416  0.0677] 1.2375 0.9999
##  word.NAKED           0.0136    0.0382 [-0.0511  0.0761] 1.5133 0.9996
##  word.NAME            0.0136    0.0248 [-0.0270  0.0519] 1.6539 1.0000
##  word.NAMES           0.0331    0.0364 [-0.0202  0.0960] 1.4296 0.9992
##  word.NAVY           -0.0343    0.0342 [-0.0921  0.0198] 1.4174 0.9992
##  word.NEIGHBORING    -0.0030    0.0404 [-0.0718  0.0596] 1.7899 0.9995
##  word.NEIGHBOUR      -0.0261    0.0387 [-0.0845  0.0347] 1.4357 1.0001
##  word.NEIGHBOURS      0.0007    0.0378 [-0.0600  0.0613] 1.4200 0.9998
##  word.NICKNAME        0.0089    0.0405 [-0.0547  0.0734] 1.6662 0.9996
##  word.NOWADAYS       -0.0294    0.0336 [-0.0804  0.0260] 1.5557 1.0000
##  word.OCCASIONS      -0.0037    0.0385 [-0.0645  0.0597] 1.5673 0.9995
##  word.OKAY           -0.0109    0.0113 [-0.0293  0.0075] 1.1170 0.9993
##  word.ORIGINATORS     0.0248    0.0392 [-0.0368  0.0864] 1.4224 1.0002
##  word.OVERRATED      -0.0048    0.0391 [-0.0719  0.0545] 1.4344 1.0007
##  word.PAGE            0.0380    0.0388 [-0.0202  0.1002] 1.2855 1.0000
##  word.PAGES          -0.0045    0.0405 [-0.0670  0.0634] 1.4538 1.0006
##  word.PAID            0.0180    0.0388 [-0.0409  0.0869] 1.3638 0.9997
##  word.PAINT           0.0232    0.0378 [-0.0425  0.0768] 1.5633 0.9994
##  word.PAINTED        -0.0033    0.0408 [-0.0671  0.0618] 1.5897 0.9997
##  word.PAINTING       -0.0167    0.0384 [-0.0778  0.0433] 1.3621 0.9999
##  word.PAPER           0.0161    0.0293 [-0.0335  0.0599] 1.5502 0.9994
##  word.PAY            -0.0116    0.0332 [-0.0635  0.0387] 1.7512 0.9996
##  word.PAYING         -0.0017    0.0371 [-0.0638  0.0577] 1.7449 0.9997
##  word.PLACE           0.0309    0.0268 [-0.0157  0.0708] 1.6853 0.9998
##  word.PLACES         -0.0011    0.0391 [-0.0674  0.0606] 1.4915 0.9995
##  word.PLATE           0.0079    0.0410 [-0.0545  0.0722] 1.6673 0.9993
##  word.PLAY            0.0417    0.0238 [ 0.0019  0.0786] 1.3641 0.9998
##  word.PLAYED          0.0479    0.0361 [-0.0075  0.1069] 1.3535 0.9996
##  word.PLAYER         -0.0010    0.0409 [-0.0683  0.0626] 1.6267 0.9997
##  word.PLAYING         0.0316    0.0318 [-0.0170  0.0806] 1.3284 0.9996
##  word.PLAYS          -0.0042    0.0392 [-0.0722  0.0569] 1.7197 0.9992
##  word.PLAYTIME       -0.0091    0.0363 [-0.0669  0.0537] 1.6352 0.9992
##  word.PORTRAY        -0.0184    0.0379 [-0.0778  0.0440] 1.3968 0.9996
##  word.PRAY            0.0115    0.0396 [-0.0528  0.0719] 1.6183 1.0002
##  word.PRAYED          0.0086    0.0399 [-0.0580  0.0727] 1.3796 0.9997
##  word.PRAYING         0.0049    0.0391 [-0.0602  0.0665] 1.2773 1.0011
##  word.RACE            0.0063    0.0355 [-0.0527  0.0622] 1.6357 0.9996
##  word.RACIAL          0.0004    0.0398 [-0.0631  0.0649] 1.5122 0.9992
##  word.RACING          0.0274    0.0227 [-0.0092  0.0663] 1.4771 0.9995
##  word.RACISM         -0.0085    0.0405 [-0.0747  0.0529] 1.4066 0.9994
##  word.RACIST         -0.0037    0.0304 [-0.0540  0.0423] 1.6841 0.9995
##  word.RAIN            0.0344    0.0398 [-0.0289  0.1019] 1.3893 1.0001
##  word.RAINBOW        -0.0165    0.0407 [-0.0796  0.0497] 1.6200 0.9996
##  word.RAINY          -0.0218    0.0381 [-0.0784  0.0446] 1.4346 1.0004
##  word.RAISED         -0.0170    0.0353 [-0.0771  0.0365] 1.7890 0.9994
##  word.RAISING        -0.0051    0.0260 [-0.0446  0.0368] 1.6423 0.9994
##  word.RAPE            0.0066    0.0406 [-0.0563  0.0738] 1.3102 1.0009
##  word.RELATED        -0.0123    0.0396 [-0.0733  0.0523] 1.6889 0.9992
##  word.RELATIONSHIP   -0.0096    0.0322 [-0.0606  0.0447] 1.4280 1.0006
##  word.RETALIATING     0.0004    0.0383 [-0.0625  0.0618] 1.3456 1.0000
##  word.ROLLERBLADES    0.0115    0.0382 [-0.0475  0.0755] 1.7192 0.9994
##  word.SAFE           -0.0148    0.0354 [-0.0684  0.0398] 1.5520 0.9992
##  word.SAFETY         -0.0227    0.0273 [-0.0650  0.0198] 1.2822 1.0006
##  word.SAINSBURYS     -0.0011    0.0376 [-0.0671  0.0608] 1.5999 0.9993
##  word.SAKE           -0.0227    0.0389 [-0.0861  0.0381] 1.6225 1.0000
##  word.SAME            0.0313    0.0130 [ 0.0095  0.0515] 1.4314 1.0003
##  word.SAVE            0.0045    0.0397 [-0.0587  0.0689] 1.4186 1.0001
##  word.SAVED          -0.0126    0.0396 [-0.0767  0.0522] 1.7185 0.9998
##  word.SAY             0.0069    0.0126 [-0.0131  0.0274] 1.0961 0.9994
##  word.SAYING          0.0340    0.0158 [ 0.0077  0.0589] 1.2138 0.9996
##  word.SEPARATED       0.0185    0.0394 [-0.0465  0.0840] 1.6273 1.0000
##  word.SHAKING         0.0041    0.0402 [-0.0626  0.0665] 1.3636 0.9992
##  word.SHAPE          -0.0068    0.0376 [-0.0667  0.0556] 1.4261 0.9991
##  word.SITUATION      -0.0130    0.0368 [-0.0709  0.0475] 1.5110 0.9994
##  word.SITUATIONS     -0.0032    0.0379 [-0.0663  0.0576] 1.5545 0.9991
##  word.SNAKE           0.0028    0.0379 [-0.0577  0.0701] 1.5319 0.9996
##  word.SPACE          -0.0128    0.0374 [-0.0743  0.0468] 1.6334 0.9998
##  word.SPRAYING        0.0016    0.0398 [-0.0620  0.0651] 1.4071 0.9993
##  word.STAGES         -0.0157    0.0388 [-0.0768  0.0484] 1.1752 0.9995
##  word.STATEMENT      -0.0106    0.0407 [-0.0764  0.0569] 1.6205 0.9994
##  word.STATION        -0.0157    0.0398 [-0.0785  0.0521] 1.5954 0.9994
##  word.STAY            0.0254    0.0249 [-0.0154  0.0632] 1.4726 0.9994
##  word.STAYED         -0.0290    0.0245 [-0.0663  0.0109] 1.4135 0.9996
##  word.STAYING         0.0212    0.0339 [-0.0336  0.0744] 1.6117 0.9998
##  word.STEAK           0.0144    0.0401 [-0.0507  0.0744] 1.5986 1.0003
##  word.STRAIGHT        0.0426    0.0312 [-0.0071  0.0929] 1.2336 0.9997
##  word.SUNDAYS        -0.0234    0.0395 [-0.0845  0.0430] 1.0003 1.0007
##  word.TABLE          -0.0322    0.0206 [-0.0656 -0.0010] 1.1729 0.9999
##  word.TAKE            0.0305    0.0181 [ 0.0009  0.0593] 1.4356 0.9996
##  word.TAKEAWAY        0.0051    0.0403 [-0.0576  0.0649] 1.4016 0.9998
##  word.TAKEN          -0.0050    0.0413 [-0.0731  0.0588] 1.6638 0.9994
##  word.TAKES          -0.0054    0.0396 [-0.0658  0.0613] 1.3856 0.9998
##  word.TAKING          0.0237    0.0341 [-0.0338  0.0793] 1.3700 0.9994
##  word.TEENAGERS       0.0122    0.0383 [-0.0452  0.0743] 1.6509 0.9992
##  word.THEY           -0.0194    0.0091 [-0.0346 -0.0060] 0.9974 1.0001
##  word.TODAY          -0.0040    0.0284 [-0.0489  0.0450] 1.6425 1.0000
##  word.TRAINED         0.0106    0.0399 [-0.0557  0.0722] 1.5854 0.9994
##  word.TRAINERS        0.0097    0.0400 [-0.0540  0.0704] 1.2942 1.0004
##  word.TUESDAY         0.0014    0.0392 [-0.0607  0.0645] 1.5745 0.9996
##  word.VIBRATING      -0.0058    0.0411 [-0.0634  0.0619] 1.5566 0.9995
##  word.WAIT            0.0449    0.0169 [ 0.0156  0.0699] 1.4649 0.9998
##  word.WAITED          0.0334    0.0397 [-0.0277  0.1061] 1.5971 0.9994
##  word.WAITING         0.0067    0.0333 [-0.0453  0.0617] 1.3866 0.9998
##  word.WAKE            0.0120    0.0373 [-0.0469  0.0727] 1.8371 0.9991
##  word.WAKES          -0.0076    0.0412 [-0.0778  0.0530] 1.8014 0.9995
##  word.WASTING        -0.0044    0.0396 [-0.0662  0.0574] 1.4634 0.9994
##  word.WAY             0.0405    0.0198 [ 0.0104  0.0722] 1.3409 0.9997
##  word.WAYS            0.0131    0.0376 [-0.0476  0.0750] 1.7557 1.0000
##  word.WEDNESDAY       0.0044    0.0401 [-0.0580  0.0694] 1.7150 0.9993
##  word.YESTERDAY      -0.0062    0.0387 [-0.0656  0.0598] 1.5298 0.9995
##    mcse
##  0.0002
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0006
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0003
##  0.0005
##  0.0002
##  0.0005
##  0.0006
##  0.0002
##  0.0005
##  0.0005
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0004
##  0.0006
##  0.0002
##  0.0002
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0003
##  0.0004
##  0.0006
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0006
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0003
##  0.0005
##  0.0003
##  0.0005
##  0.0006
##  0.0003
##  0.0005
##  0.0005
##  0.0005
##  0.0006
##  0.0005
##  0.0003
##  0.0005
##  0.0005
##  0.0003
##  0.0005
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0003
##  0.0005
##  0.0003
##  0.0003
##  0.0003
##  0.0005
##  0.0005
##  0.0004
##  0.0005
##  0.0003
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0003
##  0.0006
##  0.0005
##  0.0006
##  0.0005
##  0.0005
##  0.0006
##  0.0005
##  0.0005
##  0.0005
##  0.0006
##  0.0004
##  0.0005
##  0.0005
##  0.0003
##  0.0005
##  0.0004
##  0.0005
##  0.0003
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0003
##  0.0004
##  0.0004
##  0.0005
##  0.0005
##  0.0003
##  0.0005
##  0.0005
##  0.0004
##  0.0005
##  0.0004
##  0.0005
##  0.0003
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0004
##  0.0006
##  0.0002
##  0.0005
##  0.0005
##  0.0005
##  0.0006
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0004
##  0.0004
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0003
##  0.0004
##  0.0006
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0006
##  0.0005
##  0.0005
##  0.0005
##  0.0003
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0004
##  0.0004
##  0.0005
##  0.0005
##  0.0004
##  0.0005
##  0.0005
##  0.0005
##  0.0003
##  0.0005
##  0.0005
##  0.0002
##  0.0005
##  0.0006
##  0.0002
##  0.0002
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0005
##  0.0006
##  0.0005
##  0.0005
##  0.0005
##  0.0003
##  0.0003
##  0.0004
##  0.0006
##  0.0005
##  0.0006
##  0.0003
##  0.0002
##  0.0005
##  0.0006
##  0.0005
##  0.0004
##  0.0006
##  0.0001
##  0.0004
##  0.0006
##  0.0005
##  0.0005
##  0.0005
##  0.0002
##  0.0006
##  0.0004
##  0.0004
##  0.0005
##  0.0005
##  0.0002
##  0.0005
##  0.0005
##  0.0016
## 
## ## Random effect (Intercept: word.AIN)
## 
##             estimate std.error          HDI(89%)  ratio rhat  mcse
##  word.AIN.T   0.0055    0.0378 [-0.0582  0.0686] 1.3002    1 5e-04
## 
## ## Random effect (Intercept: word.ASIAN)
## 
##               estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word.ASIAN.S  -0.0099    0.0407 [-0.0792  0.0494] 1.3131 0.9997 5e-04
## 
## ## Random effect (Intercept: word.BABY)
## 
##              estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word.BABY.S  -0.0128    0.0388 [-0.0716  0.0479] 1.2552 0.9998 5e-04
## 
## ## Random effect (Intercept: word.BIRTHDAY)
## 
##                  estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word.BIRTHDAY.S  -0.0218    0.0368 [-0.0799  0.0353] 1.3781 0.9994 4e-04
## 
## ## Random effect (Intercept: word.GATE)
## 
##              estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word.GATE.S   0.0091    0.0397 [-0.0537  0.0726] 1.3074 0.9991 6e-04
## 
## ## Random effect (Intercept: word.INVESTIGATOR)
## 
##                      estimate std.error          HDI(89%)  ratio   rhat
##  word.INVESTIGATOR.S  -0.0095    0.0394 [-0.0711  0.0555] 1.6221 0.9992
##   mcse
##  5e-04
## 
## ## Random effect (Intercept: word.M)
## 
##             estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word.M.KAY  -0.0214    0.0383 [-0.0877  0.0390] 1.5482 1.0002 5e-04
## 
## ## Random effect (Intercept: word.NAME)
## 
##              estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word.NAME.S   0.0142    0.0394 [-0.0536  0.0772] 1.4267 0.9997 5e-04
## 
## ## Random effect (Intercept: word.PAPER)
## 
##               estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word.PAPER.S   0.0133    0.0383 [-0.0544  0.0705] 1.3959 0.9992 5e-04
## 
## ## Random effect (Intercept: word.THEY)
## 
##               estimate std.error          HDI(89%)  ratio   rhat  mcse
##  word.THEY.D   -0.0163    0.0414 [-0.0779  0.0514] 1.2623 0.9994 5e-04
##  word.THEY.VE  -0.0088    0.0365 [-0.0657  0.0493] 1.4247 0.9994 4e-04
## 
## ## Random effect (Intercept: word.TODAY)
## 
##               estimate std.error          HDI(89%) ratio   rhat  mcse
##  word.TODAY.S  -0.0083     0.041 [-0.0703  0.0592] 1.682 0.9997 5e-04</code></pre>
<p>The authors of this other package recommend (a) using 89% credible interval, rather than 95%, for not-so-big data; and (b) looking at where the majority of the posterior distribution lies, not the 95%/89% that is symmetrical about the point estimate. See in the plots how the blue area is not necessarily symmetrical about the point estimate:</p>
<pre class="r"><code>posterior1 &lt;- as.matrix(model1.1)

#dimnames(posterior)
mcmc_areas(posterior1,
           pars = c(&quot;b_Intercept&quot;, 
                    &quot;b_Log_dur_z&quot;,
                    &quot;b_agechild&quot;, 
                    &quot;b_SexF&quot;, 
                    &quot;b_agechild:SexF&quot;),
           prob=0.89)</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-20-1.png" width="672" /> This plot gives us an idea of how confident to be about the effects of different predictors. Not only are the coefficients for age, sex and their interaction close to zero, the posterior distributions over these parameters are pretty flat compared with e.g. the posterior distribution for duration.</p>
<p>This tidy_stan also gives us all the random intercepts, which is pretty nice, especially the ones for speakers.</p>
<div id="plotting-bayesian-output" class="section level3">
<h3>Plotting Bayesian output</h3>
<p>Plot marginal effects</p>
<pre class="r"><code>marg1 &lt;- marginal_effects(model1.1, ask = FALSE)

plot(marg1, plot = FALSE)[[2]]</code></pre>
<p><img src="icphs_regression_files/figure-html/fig1-1.png" width="672" /></p>
<p>Save the numbers needed for an interaction plot to a tibble:</p>
<pre class="r"><code>faceF1 &lt;- as.data.frame(marg1[4])</code></pre>
<p>Set up the colour palette and the position dodge that we’ll be using for all of these plots:</p>
<pre class="r"><code>binary &lt;- c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;)

pd &lt;- position_dodge(0.9)</code></pre>
<pre class="r"><code>o1 &lt;- ggplot(data = face, aes(x = age, y = normF1_20, fill = Sex)) + 
  geom_violin(color = NA) + 
  geom_boxplot(width = 0.3, color = &quot;#999999&quot;, position = pd) +
  theme_minimal() + 
  scale_fill_manual(values = binary) +
   theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.title = element_text(size = rel(2.5)),
        legend.text = element_text(size = rel(2.5)),
        axis.title = element_text(size = rel(3.5)), 
        axis.text = element_text(size = rel(3)), 
        plot.title = element_text(size = rel(3.5)), 
        plot.margin=unit(c(0.7,0.7,0.7,0.7),&quot;cm&quot;)) +
  labs(x = &quot;Age&quot;, y = &quot;Normalized F1 at 20%&quot;, 
       title = &quot;FACE: normalized F1 at onset (20%)&quot;)

o1</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-23-1.png" width="768" /></p>
<pre class="r"><code>str(faceF1)</code></pre>
<pre><code>## &#39;data.frame&#39;:    4 obs. of  13 variables:
##  $ age.Sex.age        : Factor w/ 2 levels &quot;adolescent&quot;,&quot;child&quot;: 1 1 2 2
##  $ age.Sex.Sex        : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 1 2 1 2
##  $ age.Sex.normF1_20  : num  0.986 0.986 0.986 0.986
##  $ age.Sex.Log_dur_z  : num  -0.00818 -0.00818 -0.00818 -0.00818
##  $ age.Sex.participant: logi  NA NA NA NA
##  $ age.Sex.word       : logi  NA NA NA NA
##  $ age.Sex.cond__     : Factor w/ 1 level &quot;1&quot;: 1 1 1 1
##  $ age.Sex.effect1__  : Factor w/ 2 levels &quot;adolescent&quot;,&quot;child&quot;: 1 1 2 2
##  $ age.Sex.effect2__  : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 1 2 1 2
##  $ age.Sex.estimate__ : num  1.018 1.017 0.961 0.949
##  $ age.Sex.se__       : num  0.0242 0.0241 0.0245 0.0246
##  $ age.Sex.lower__    : num  0.969 0.968 0.913 0.898
##  $ age.Sex.upper__    : num  1.066 1.067 1.012 0.997</code></pre>
<p>Define columns to keep:</p>
<pre class="r"><code>keep1 &lt;- c(&quot;age.Sex.age&quot;, &quot;age.Sex.Sex&quot;, &quot;age.Sex.estimate__&quot;, &quot;age.Sex.se__&quot;, &quot;age.Sex.lower__&quot;, &quot;age.Sex.upper__&quot;)</code></pre>
<p>Perform “select”:</p>
<pre class="r"><code>faceF1_2 &lt;- faceF1 %&gt;% select(keep1)</code></pre>
<p>Rename columns:</p>
<pre class="r"><code>colnames(faceF1_2) &lt;- c(&quot;age&quot;,&quot;Sex&quot;,&quot;normF1_20&quot;, &quot;SE&quot;,&quot;lower&quot;, &quot;upper&quot;)</code></pre>
<p>Add error bars</p>
<pre class="r"><code>o2 &lt;- o1 + geom_point(data = faceF1_2, aes(x=age, y=normF1_20, group = Sex),
                      position = pd,
                      size = 4) +
  geom_errorbar(data = faceF1_2, 
                aes(ymin=lower, ymax=upper), 
                width=.1,
                size=1,
                position = pd)

o2</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-28-1.png" width="768" /></p>
</div>
</div>
<div id="face-trajectory" class="section level2">
<h2>FACE trajectory</h2>
<p>As with FACE F1, note that I have done exploratory analysis of the distribution of the dependent variable in relation to its predictors, but I am not going to do all that here because it would take up lots of space…</p>
<p>Check distribution of TL in relation to duration (log10-transformed and Z-scored), age and sex:</p>
<pre class="r"><code>ggplot(face, aes(x=Log_dur_z)) + geom_histogram(bins = 100)</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<pre class="r"><code>ggplot(face, aes(x=age, y = norm_TL, fill = Sex)) + geom_violin()</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-29-2.png" width="672" /></p>
<p>Define the priors:</p>
<pre class="r"><code>priors2 &lt;- c(set_prior(&quot;normal(0.75, 0.75)&quot;, class = &quot;Intercept&quot;), # set a prior for the intercept
              set_prior(&quot;normal(0, 2)&quot;, class = &quot;b&quot;, coef = &quot;Log_dur_z&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;b&quot;, coef = &quot;agechild&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;b&quot;, coef = &quot;SexF&quot;),
              set_prior(&quot;normal(0, 1.5)&quot;, class = &quot;b&quot;, coef = &quot;agechild:SexF&quot;), #2*age effect to allow for complete reversal
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group=&quot;participant&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group=&quot;word&quot;)
              )</code></pre>
<p>And build the model:</p>
<pre class="r"><code>model2 &lt;- brm(norm_TL ~ Log_dur_z + age*Sex + (1|participant) + (1|word), data = face, prior = priors2)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;7b293a91127b74207a6959a0792c2ea1&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000732 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 7.32 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 29.4748 seconds (Warm-up)
## Chain 1:                10.8748 seconds (Sampling)
## Chain 1:                40.3496 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;7b293a91127b74207a6959a0792c2ea1&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.00042 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 4.2 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 39.9077 seconds (Warm-up)
## Chain 2:                11.6134 seconds (Sampling)
## Chain 2:                51.5212 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;7b293a91127b74207a6959a0792c2ea1&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0.000441 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 4.41 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 32.3618 seconds (Warm-up)
## Chain 3:                7.37833 seconds (Sampling)
## Chain 3:                39.7401 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;7b293a91127b74207a6959a0792c2ea1&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0.000167 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.67 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 26.2237 seconds (Warm-up)
## Chain 4:                5.44859 seconds (Sampling)
## Chain 4:                31.6723 seconds (Total)
## Chain 4:</code></pre>
<p>Check the model summary</p>
<pre class="r"><code>summary(model2)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: norm_TL ~ Log_dur_z + age * Sex + (1 | participant) + (1 | word) 
##    Data: face (Number of observations: 1935) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~participant (Number of levels: 28) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.04      0.01     0.03     0.06       1628 1.00
## 
## ~word (Number of levels: 246) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.05      0.01     0.03     0.06       1394 1.00
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept         0.33      0.02     0.29     0.36       2454 1.00
## Log_dur_z         0.06      0.00     0.05     0.07       6457 1.00
## agechild          0.07      0.03     0.02     0.12       2396 1.00
## SexF              0.07      0.03     0.03     0.13       1997 1.00
## agechild:SexF    -0.11      0.04    -0.18    -0.04       2166 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     0.16      0.00     0.16     0.17       6430 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>So, as regards the trajectory of FACE:</p>
<ul>
<li><p>longer duration = greater Trajectory Length i.e. more movement in the diphthong. This is to be expected really with diphthongs. The 95% credible interval is narrow, bounded at 0.05 and 0.07, so we are pretty confident that duration has a positive relationship with TL.</p></li>
<li><p>Being a child (male) as opposed to adolescent (male) also predicts longer trajectory. The coefficients and 95% credible intervals are actually identical for the age and sex predictors. The 95% credible interval for each is (0.02, 0.12).</p></li>
<li><p>There is an interaction of age and sex that counterbalances the age effect. 0.07 + -0.1 = -0.03, so the difference between adolescent males and female children is estimated at -0.03? i.e. female children have smaller TL i.e. are more monophthongal than male adolescents?</p></li>
</ul>
<p>The tidy_stan summary:</p>
<pre class="r"><code>tidy_stan(model2, prob = 0.89, type = &quot;all&quot;, digits = 8)</code></pre>
<pre><code>## Warning in stddev/sqrt(ess): longer object length is not a multiple of
## shorter object length</code></pre>
<pre><code>## 
## # Summary Statistics of Stan-Model
## 
## ## Fixed effects:
## 
##                   estimate  std.error                  HDI(89%)     ratio
##  Intercept      0.32595094 0.01796172 [ 0.29761433  0.35667348] 0.6136211
##  Log_dur_z      0.05809540 0.00468540 [ 0.05028544  0.06549762] 1.6142458
##  Log_dur_z      0.05809540 0.00468540 [ 0.05028544  0.06549762] 1.6142458
##  agechild       0.07192782 0.02564824 [ 0.03256438  0.11400229] 0.5990117
##  SexF           0.07442732 0.02342250 [ 0.03028656  0.11104853] 0.4991847
##  agechild.SexF -0.10486529 0.03491867 [-0.16005002 -0.04524390] 0.5415877
##       rhat       mcse
##  1.0010357 0.00037288
##  0.9994511 0.00005924
##  0.9994511 0.00005924
##  1.0007790 0.00052945
##  1.0016280 0.00056046
##  1.0010868 0.00077036
## 
## ## Random effect (Intercept: participant)
## 
##                           estimate  std.error                  HDI(89%)
##  participant.Amanda     0.00641164 0.02356715 [-0.03454114  0.04170349]
##  participant.CB        -0.06989466 0.02308298 [-0.10751498 -0.03301291]
##  participant.Chantelle -0.03098754 0.02151848 [-0.06818200  0.00228741]
##  participant.ChrisB    -0.00471823 0.02096182 [-0.03798929  0.02984881]
##  participant.F1         0.00587553 0.02633890 [-0.03304798  0.04997001]
##  participant.F10       -0.02299696 0.02187795 [-0.05660387  0.01171362]
##  participant.F3         0.00111723 0.02170143 [-0.03347466  0.03757411]
##  participant.F4         0.01466778 0.02798282 [-0.02669935  0.06020602]
##  participant.F7        -0.00100181 0.02508509 [-0.04406488  0.03676539]
##  participant.F8        -0.03483905 0.02428275 [-0.07380303  0.00456395]
##  participant.F9         0.03753817 0.02274022 [ 0.00115828  0.07424905]
##  participant.Ibrahim    0.05362187 0.02325613 [ 0.01614317  0.08904790]
##  participant.Jessica    0.03958701 0.02299782 [ 0.00229582  0.07548031]
##  participant.Lola      -0.02658058 0.02039303 [-0.06024363  0.00630012]
##  participant.Lucy      -0.03774833 0.02323352 [-0.07465860  0.00196752]
##  participant.M1         0.00860388 0.02358262 [-0.02840387  0.04727049]
##  participant.M3         0.01933299 0.02409788 [-0.01947901  0.06014452]
##  participant.M4        -0.02376154 0.02153077 [-0.05823612  0.01192354]
##  participant.M5         0.05815267 0.02369760 [ 0.02168130  0.09511172]
##  participant.M6        -0.02141895 0.02363413 [-0.05808903  0.01616829]
##  participant.M7        -0.01170005 0.02466834 [-0.04958444  0.02960747]
##  participant.M8        -0.02715729 0.02542648 [-0.06857182  0.01347119]
##  participant.Matisse   -0.00378644 0.02218062 [-0.03838050  0.03297228]
##  participant.Omar      -0.03867312 0.02361303 [-0.07316039  0.00176246]
##  participant.Sami       0.03529591 0.02291831 [-0.00095818  0.07171117]
##  participant.Shantel    0.04906233 0.02315093 [ 0.01216411  0.08668456]
##  participant.Tariq      0.03540264 0.02249744 [ 0.00043961  0.07185472]
##  participant.ZR        -0.01579878 0.02327523 [-0.05294755  0.02423172]
##      ratio      rhat       mcse
##  0.9772971 1.0001148 0.00041575
##  0.8294746 1.0011076 0.00043523
##  0.7330230 1.0004043 0.00038585
##  0.8240558 1.0003811 0.00027272
##  1.5293706 0.9993567 0.00042081
##  0.9851096 0.9996499 0.00034579
##  1.0062783 0.9995642 0.00029548
##  1.4381226 0.9995027 0.00038115
##  1.3164230 0.9998218 0.00036565
##  1.2054182 0.9992864 0.00038893
##  1.0070963 0.9995766 0.00039569
##  0.8551304 1.0012197 0.00037971
##  0.9365150 1.0005636 0.00043932
##  0.6936453 1.0002371 0.00034099
##  0.9282645 0.9996697 0.00037989
##  1.0058263 0.9995179 0.00035749
##  1.1361556 0.9997060 0.00042190
##  0.8865686 0.9993855 0.00038070
##  0.8243278 0.9996609 0.00035728
##  1.0699819 0.9994004 0.00034502
##  1.1700970 0.9994717 0.00037170
##  1.1527390 1.0000692 0.00044867
##  0.8272221 1.0005549 0.00037139
##  0.9254530 1.0003782 0.00041512
##  0.8251046 1.0005177 0.00036739
##  0.9807286 1.0001140 0.00039498
##  0.8959895 1.0002430 0.00035289
##  1.0283797 1.0003320 0.00025062
## 
## ## Random effect (Intercept: word...APRIL..)
## 
##                    estimate  std.error                  HDI(89%)    ratio
##  word...APRIL.. -0.01601399 0.04273571 [-0.08851891  0.05400618] 2.359483
##       rhat       mcse
##  0.9992006 0.00045152
## 
## ## Random effect (Intercept: word...EIGHTEENTH..)
## 
##                        estimate  std.error                  HDI(89%)
##  word...EIGHTEENTH.. 0.00273113 0.04290128 [-0.06905585  0.07179536]
##     ratio      rhat       mcse
##  2.429263 0.9998056 0.00042022
## 
## ## Random effect (Intercept: word...FACE..)
## 
##                   estimate  std.error                  HDI(89%)    ratio
##  word...FACE.. -0.00411386 0.04071352 [-0.07303071  0.06281602] 2.651333
##      rhat       mcse
##  1.000031 0.00045457
## 
## ## Random effect (Intercept: word)
## 
##                        estimate  std.error                  HDI(89%)
##  word.A             -0.01366803 0.02277441 [-0.05010689  0.02355437]
##  word.ABBREVIATIONS  0.00240873 0.04319090 [-0.06665140  0.07379409]
##  word.ABLE          -0.00187648 0.04296468 [-0.07308147  0.06509473]
##  word.ACE           -0.00199240 0.04040922 [-0.06794176  0.06920494]
##  word.ACHE          -0.01753017 0.04334851 [-0.09493752  0.04513138]
##  word.ACQUAINTANCES  0.00397091 0.04247926 [-0.06493892  0.07441666]
##  word.AEROPLANE      0.00809415 0.04163549 [-0.06126214  0.07608714]
##  word.AGE            0.01934534 0.03303269 [-0.03542595  0.07156816]
##  word.ALWAYS         0.01897104 0.04068812 [-0.05038727  0.08143905]
##  word.AMAZINGLY     -0.01165590 0.04056069 [-0.07697375  0.06035834]
##  word.ANYWAY         0.01461666 0.03961968 [-0.05340277  0.07750812]
##  word.ANYWAYS       -0.00761020 0.04399996 [-0.07793912  0.06246620]
##  word.APRIL         -0.01115519 0.03503539 [-0.06782847  0.04742817]
##  word.ASIAN         -0.01145675 0.03993925 [-0.07772845  0.05488897]
##  word.ATE            0.00349188 0.04038883 [-0.06239039  0.06672736]
##  word.AWAY           0.04518774 0.03029067 [-0.00106404  0.09547306]
##  word.BABIES        -0.00142400 0.04291816 [-0.07683538  0.06531211]
##  word.BABY           0.01780351 0.01959783 [-0.01293093  0.05084307]
##  word.BAIT           0.01787543 0.04152159 [-0.05162101  0.08275508]
##  word.BASED          0.01588409 0.04207215 [-0.04885451  0.08073790]
##  word.BASICALLY      0.02638542 0.02321399 [-0.01309198  0.06337899]
##  word.BECAME        -0.01595506 0.04158087 [-0.08394428  0.04795989]
##  word.BEHAVIOR      -0.00241602 0.04458320 [-0.07457593  0.06460533]
##  word.BEHAVIOUR     -0.01330565 0.04028560 [-0.08448609  0.04791454]
##  word.BIRTHDAY      -0.01095585 0.03466279 [-0.06902083  0.04558046]
##  word.BLAME         -0.00343261 0.04150707 [-0.07655400  0.06168332]
##  word.BRAVE          0.00416076 0.04418823 [-0.06504904  0.07412802]
##  word.BREAK          0.02170663 0.04155321 [-0.04706691  0.09286775]
##  word.BREAKING      -0.00234090 0.04179895 [-0.07645125  0.06665989]
##  word.CAKE          -0.01878272 0.02237087 [-0.05541166  0.01485070]
##  word.CAME          -0.03862112 0.02416854 [-0.07400382  0.00186419]
##  word.CAPABLE       -0.01044473 0.04463131 [-0.08472591  0.05847842]
##  word.CASE          -0.01517140 0.04313226 [-0.08817237  0.05061818]
##  word.CELEBRATE     -0.01129263 0.04135776 [-0.08029830  0.05866929]
##  word.CELEBRATING   -0.00310575 0.04169425 [-0.07379305  0.06488489]
##  word.CHANGE         0.00992253 0.02892553 [-0.03744548  0.05674734]
##  word.CHANGED       -0.01939996 0.04106473 [-0.08682105  0.04881319]
##  word.CHANGES       -0.00532608 0.04211616 [-0.07821476  0.05916645]
##  word.CHASE         -0.02285143 0.04242901 [-0.08660148  0.04969909]
##  word.CHASED        -0.01516559 0.04024370 [-0.08212155  0.05214707]
##  word.CHASING       -0.00013734 0.04325232 [-0.06999521  0.07026280]
##  word.CLAIM          0.00526238 0.04220022 [-0.06215835  0.07608823]
##  word.CLAIMED       -0.00367222 0.04304247 [-0.07770939  0.06371443]
##  word.CLAIMING       0.00346616 0.04051964 [-0.06440254  0.07060880]
##  word.COMMUNICATE   -0.01097967 0.04310416 [-0.08225033  0.06063122]
##  word.COMMUNICATION -0.00227338 0.04087418 [-0.06828738  0.06420319]
##  word.CONVERSATION   0.01692624 0.03896484 [-0.04575006  0.08322949]
##  word.CONVERSATIONS -0.01137451 0.04371867 [-0.08091253  0.05803388]
##  word.CRAZY          0.09651839 0.03894512 [ 0.03953006  0.15938932]
##  word.CREATE        -0.00087959 0.04152944 [-0.06869156  0.07006816]
##  word.CREATED       -0.01368429 0.04363404 [-0.08406103  0.05519914]
##  word.CREATING      -0.00484977 0.04270983 [-0.07493064  0.06212331]
##  word.CUPCAKE       -0.02903502 0.04154102 [-0.09843369  0.03411347]
##  word.DAISY         -0.02629949 0.03909627 [-0.09526776  0.03262934]
##  word.DATE          -0.00176744 0.04044547 [-0.06623821  0.06483301]
##  word.DATES         -0.00556433 0.04070696 [-0.07189344  0.05567543]
##  word.DAY           -0.05131778 0.02567324 [-0.09230606 -0.01201955]
##  word.DAYLIGHT       0.05260029 0.04600251 [-0.02512785  0.12908479]
##  word.DAYS          -0.00789137 0.02691787 [-0.05064343  0.03334054]
##  word.DISABLED       0.00234092 0.04131107 [-0.06307480  0.06986333]
##  word.DONATED       -0.01189860 0.04232788 [-0.08210231  0.05887026]
##  word.EIGHT          0.02197370 0.02476432 [-0.01937946  0.05781052]
##  word.EIGHTH        -0.00629924 0.04119546 [-0.06946832  0.06092412]
##  word.EIGHTY         0.01216888 0.04002448 [-0.05945573  0.07692426]
##  word.ELABORATE     -0.00801133 0.04274130 [-0.07879879  0.05876548]
##  word.ELABORATED     0.00561425 0.04356942 [-0.06104476  0.07943156]
##  word.ESCALATES     -0.01713874 0.04279690 [-0.08870361  0.04801074]
##  word.ESCAPED       -0.00519384 0.03144551 [-0.05694589  0.04494784]
##  word.ESTATE        -0.00319018 0.03974279 [-0.06656242  0.06080113]
##  word.EXPLAIN       -0.00820675 0.03888862 [-0.07029620  0.06232999]
##  word.FACE           0.02657529 0.02883403 [-0.02189781  0.07330529]
##  word.FACED         -0.01003944 0.04300209 [-0.07846727  0.06226040]
##  word.FACES         -0.00880183 0.04261628 [-0.07560860  0.06365824]
##  word.FAKE           0.02286856 0.03811955 [-0.03662081  0.08617664]
##  word.FAMOUS        -0.00275002 0.04028639 [-0.06856463  0.06816395]
##  word.FAVORITE      -0.00539366 0.04206845 [-0.08233059  0.06018980]
##  word.FAVOURITE     -0.01182483 0.04120685 [-0.07800338  0.05804801]
##  word.FLAMES        -0.00619169 0.04130852 [-0.07092544  0.06511615]
##  word.FLIRTATIOUS    0.00169718 0.04313781 [-0.06601186  0.07157147]
##  word.FRIDAY        -0.00576216 0.04407622 [-0.07409529  0.06994192]
##  word.GAME          -0.03826411 0.03292300 [-0.09266729  0.01075124]
##  word.GAMES         -0.02031721 0.04103237 [-0.08462468  0.04693980]
##  word.GATE           0.05388695 0.02774524 [ 0.00964542  0.09805089]
##  word.GAVE          -0.03544296 0.02840111 [-0.07840463  0.01332246]
##  word.GAY           -0.02095262 0.03189976 [-0.07081146  0.03355558]
##  word.GAYS          -0.01621768 0.04438009 [-0.08976958  0.05252625]
##  word.GENERATION     0.01654233 0.04088132 [-0.05056071  0.08412378]
##  word.GREAT          0.02491608 0.03616406 [-0.03271283  0.08838530]
##  word.GRENADA       -0.00930107 0.04142377 [-0.07876826  0.05525580]
##  word.GREY           0.08006997 0.03373520 [ 0.03008023  0.13771403]
##  word.H             -0.00960683 0.04172281 [-0.07504738  0.06080875]
##  word.HAIRSPRAY     -0.00563445 0.04318782 [-0.07884196  0.06125865]
##  word.HATE          -0.00501709 0.03930701 [-0.07132168  0.05340306]
##  word.HATED          0.00135046 0.03745931 [-0.05677885  0.06094849]
##  word.HAY            0.03967128 0.02846952 [-0.00669171  0.08294773]
##  word.HAYS          -0.00947952 0.04209963 [-0.07544354  0.05952117]
##  word.HEY            0.00880407 0.03818367 [-0.05182437  0.07525157]
##  word.HEYA           0.02008544 0.04473575 [-0.04793400  0.09132133]
##  word.HOLIDAY        0.03587573 0.04172476 [-0.03359431  0.10338751]
##  word.HOLIDAYS      -0.01405898 0.03833971 [-0.07801827  0.04952800]
##  word.INDICATE       0.00278494 0.04079317 [-0.06341850  0.07424582]
##  word.INTEGRATED    -0.01359847 0.04391883 [-0.08858806  0.05494312]
##  word.INVADING      -0.00545441 0.04145975 [-0.07079969  0.06246374]
##  word.ISOLATION      0.01430429 0.04145961 [-0.05218076  0.08285853]
##  word.ISRAELI        0.00312047 0.04369215 [-0.06607974  0.07562738]
##  word.J              0.00831688 0.03832762 [-0.05484081  0.06757405]
##  word.JAMAICAN      -0.00087150 0.04271230 [-0.07257155  0.06538403]
##  word.K             -0.01060086 0.04227869 [-0.07830411  0.05427824]
##  word.KAY           -0.07023048 0.03629049 [-0.12834302 -0.01262993]
##  word.LADIES        -0.00138232 0.04222974 [-0.07245575  0.06606692]
##  word.LADY           0.04541517 0.03160277 [-0.00147169  0.09967899]
##  word.LATE           0.00182155 0.03606902 [-0.06349687  0.05660221]
##  word.LATER          0.00668963 0.03001421 [-0.04196087  0.05495624]
##  word.LAYING         0.00099307 0.03960282 [-0.06503272  0.06761103]
##  word.MADE          -0.01445604 0.03057604 [-0.06038904  0.03304119]
##  word.MAIN          -0.00085999 0.04350326 [-0.06975803  0.06601884]
##  word.MAINLY        -0.00723253 0.03839017 [-0.06672246  0.05489286]
##  word.MAINSTREAM    -0.00588597 0.04105445 [-0.07207481  0.06243367]
##  word.MAISONETTE     0.00219360 0.04308308 [-0.06504035  0.07322202]
##  word.MAJOR         -0.01617514 0.04166110 [-0.08425088  0.04755453]
##  word.MAJORLY        0.00503004 0.04071655 [-0.06001980  0.07844508]
##  word.MAKE           0.03314833 0.02667069 [-0.01047709  0.07476227]
##  word.MAKES          0.04458078 0.03748821 [-0.01618375  0.10415840]
##  word.MAKING         0.04027502 0.03663287 [-0.01424232  0.10508919]
##  word.MATES         -0.01082095 0.04301349 [-0.07941186  0.05361913]
##  word.MAY           -0.01007708 0.04265953 [-0.08526986  0.05727620]
##  word.MAYBE         -0.03495567 0.03054526 [-0.08891953  0.01381557]
##  word.MEDICATION    -0.00538090 0.04255807 [-0.07466741  0.06461963]
##  word.MISTAKE       -0.02353772 0.03976769 [-0.09157713  0.04303326]
##  word.MISTAKES      -0.00439787 0.03831926 [-0.06423914  0.06067241]
##  word.MKAY          -0.04259578 0.04150866 [-0.10771225  0.02686750]
##  word.MONDAY        -0.00664205 0.03931914 [-0.06994931  0.05393619]
##  word.NAKED          0.02796988 0.04288334 [-0.04250001  0.10160930]
##  word.NAME          -0.03798182 0.03021167 [-0.08994811  0.00719268]
##  word.NAMES         -0.02431130 0.04202571 [-0.09269453  0.04342090]
##  word.NAVY          -0.00657141 0.03856344 [-0.07119723  0.05293502]
##  word.NEIGHBORING   -0.00257765 0.04321521 [-0.07568207  0.06824146]
##  word.NEIGHBOUR     -0.01234439 0.04242685 [-0.07987735  0.05611797]
##  word.NEIGHBOURS    -0.00646708 0.04108001 [-0.06834387  0.06616540]
##  word.NICKNAME      -0.00312813 0.04243739 [-0.07324975  0.07015025]
##  word.NOWADAYS       0.03177326 0.03979431 [-0.02857024  0.09819094]
##  word.OCCASIONS     -0.01156593 0.04264462 [-0.08100110  0.06052706]
##  word.OKAY          -0.04424732 0.01535446 [-0.06787996 -0.01951223]
##  word.ORIGINATORS   -0.00499429 0.04157684 [-0.07258195  0.06560085]
##  word.OVERRATED     -0.01459981 0.04360966 [-0.08562996  0.05324343]
##  word.PAGE           0.00719702 0.04135575 [-0.06461725  0.06903147]
##  word.PAGES          0.00861172 0.04446408 [-0.05913451  0.08206389]
##  word.PAID           0.01436029 0.04352334 [-0.05524574  0.08700821]
##  word.PAINT          0.01261468 0.03924218 [-0.04924276  0.07687800]
##  word.PAINTED        0.00089868 0.04278483 [-0.06921081  0.07120510]
##  word.PAINTING       0.02176188 0.04152814 [-0.04335945  0.09101898]
##  word.PAPER          0.01529973 0.03447706 [-0.03961034  0.07362878]
##  word.PAY           -0.02978060 0.03816820 [-0.09623312  0.03038570]
##  word.PAYING        -0.00121265 0.04175273 [-0.06428371  0.06932271]
##  word.PLACE          0.00922880 0.03219935 [-0.04194569  0.06169310]
##  word.PLACES        -0.00623411 0.04237472 [-0.07182511  0.06478819]
##  word.PLATE          0.00049800 0.04148972 [-0.07234888  0.06291796]
##  word.PLAY          -0.00126456 0.03072733 [-0.04901673  0.04761611]
##  word.PLAYED         0.01619675 0.03744427 [-0.04513187  0.07836278]
##  word.PLAYER        -0.01755972 0.04218018 [-0.08914354  0.05098837]
##  word.PLAYING       -0.03395427 0.03646645 [-0.09784169  0.02177357]
##  word.PLAYS          0.00039242 0.04278772 [-0.06795317  0.06767279]
##  word.PLAYTIME       0.02293423 0.04129185 [-0.04265380  0.08917778]
##  word.PORTRAY        0.00188150 0.03984302 [-0.06512381  0.06713581]
##  word.PRAY          -0.00544291 0.04530611 [-0.07765109  0.06906396]
##  word.PRAYED         0.02042153 0.04377711 [-0.05309278  0.09036259]
##  word.PRAYING       -0.00644574 0.04172154 [-0.07649688  0.06222557]
##  word.RACE           0.00041300 0.03901302 [-0.06650331  0.06019022]
##  word.RACIAL         0.00231952 0.04226821 [-0.06417551  0.07243708]
##  word.RACING         0.04775974 0.03164411 [-0.00017906  0.10068067]
##  word.RACISM         0.02520957 0.04287470 [-0.04697993  0.09763539]
##  word.RACIST         0.02464728 0.03547006 [-0.03181917  0.08391906]
##  word.RAIN           0.00777855 0.04303132 [-0.05890692  0.07951221]
##  word.RAINBOW       -0.01235570 0.04160083 [-0.08373082  0.05349882]
##  word.RAINY          0.01087716 0.04108462 [-0.05444680  0.08060309]
##  word.RAISED         0.00206897 0.03906263 [-0.06293069  0.06802427]
##  word.RAISING        0.02629477 0.03202865 [-0.02298650  0.07719270]
##  word.RAPE          -0.00310064 0.04043580 [-0.06702002  0.07080618]
##  word.RELATED       -0.00680280 0.04076465 [-0.07697047  0.05872550]
##  word.RELATIONSHIP   0.00540665 0.03988000 [-0.05529711  0.06916740]
##  word.RETALIATING   -0.00255418 0.04289854 [-0.07634747  0.06436559]
##  word.ROLLERBLADES  -0.01718256 0.04121605 [-0.08714515  0.05076981]
##  word.SAFE          -0.00741540 0.03522534 [-0.06802585  0.04904835]
##  word.SAFETY        -0.00630456 0.03179781 [-0.05844821  0.04341961]
##  word.SAINSBURYS     0.00173848 0.04282816 [-0.06803669  0.06918104]
##  word.SAKE          -0.02085536 0.04146359 [-0.08552507  0.04838397]
##  word.SAME           0.01635610 0.01752462 [-0.01099843  0.04375747]
##  word.SAVE          -0.00456421 0.04302629 [-0.07261160  0.06543892]
##  word.SAVED         -0.00740898 0.04393740 [-0.07695763  0.06154014]
##  word.SAY           -0.02045279 0.01660209 [-0.04963727  0.00482035]
##  word.SAYING        -0.00252506 0.02069593 [-0.03519616  0.02956989]
##  word.SEPARATED      0.03325267 0.04265574 [-0.04357131  0.09924336]
##  word.SHAKING       -0.00196584 0.04204942 [-0.07109715  0.07096614]
##  word.SHAPE         -0.00531091 0.04113313 [-0.06647977  0.06410700]
##  word.SITUATION     -0.00193889 0.04245114 [-0.06825147  0.06607969]
##  word.SITUATIONS    -0.00645695 0.04141068 [-0.07231014  0.06375978]
##  word.SNAKE          0.00810730 0.04294712 [-0.06377992  0.07549387]
##  word.SPACE          0.04814668 0.04081053 [-0.01815770  0.11351056]
##  word.SPRAYING      -0.00311660 0.04376820 [-0.07379300  0.06939737]
##  word.STAGES        -0.01633050 0.04190823 [-0.08947711  0.05051919]
##  word.STATEMENT     -0.00160415 0.04537258 [-0.07269882  0.07443155]
##  word.STATION       -0.00482302 0.04297309 [-0.07039811  0.07349404]
##  word.STAY           0.02156066 0.03160418 [-0.02799146  0.06930928]
##  word.STAYED        -0.02492100 0.02986865 [-0.07216302  0.02117385]
##  word.STAYING        0.00844059 0.03787537 [-0.05347649  0.06979088]
##  word.STEAK          0.00263894 0.04005995 [-0.06404734  0.06650724]
##  word.STRAIGHT      -0.00285405 0.03637920 [-0.06442362  0.05306767]
##  word.SUNDAYS       -0.00878181 0.04355069 [-0.07905982  0.05955017]
##  word.TABLE          0.00630886 0.02520918 [-0.03516676  0.04492121]
##  word.TAKE          -0.00656557 0.02355545 [-0.04280021  0.02981238]
##  word.TAKEAWAY      -0.00879092 0.04424739 [-0.07500278  0.06590089]
##  word.TAKEN         -0.00648624 0.04071318 [-0.07186038  0.06570589]
##  word.TAKES          0.05832402 0.04587559 [-0.01561626  0.13248673]
##  word.TAKING         0.00540148 0.03818778 [-0.05538378  0.06579821]
##  word.TEENAGERS     -0.00606469 0.04025990 [-0.07297501  0.06102986]
##  word.THEY          -0.00721130 0.01202319 [-0.02828320  0.01072308]
##  word.TODAY         -0.02403091 0.03404380 [-0.08514577  0.02797753]
##  word.TRAINED        0.00009407 0.04243966 [-0.06958105  0.07166728]
##  word.TRAINERS      -0.00540828 0.04428823 [-0.07700359  0.06565476]
##  word.TUESDAY        0.00517721 0.04127686 [-0.06754045  0.07150127]
##  word.VIBRATING     -0.01556753 0.04545028 [-0.07888908  0.05930870]
##  word.WAIT           0.06900612 0.02261371 [ 0.03480548  0.10622274]
##  word.WAITED         0.00731122 0.04312747 [-0.06070161  0.07496170]
##  word.WAITING       -0.01949445 0.03840412 [-0.08178250  0.04657089]
##  word.WAKE           0.00249416 0.04174239 [-0.06416304  0.07516901]
##  word.WAKES          0.02453865 0.04511593 [-0.05390056  0.09306608]
##  word.WASTING        0.00090124 0.04258767 [-0.06589823  0.07151973]
##  word.WAY            0.00286917 0.02504707 [-0.03908798  0.04070080]
##  word.WAYS          -0.00802367 0.04164591 [-0.07455635  0.05669820]
##  word.WEDNESDAY     -0.00100351 0.04066476 [-0.07092779  0.06827044]
##  word.YESTERDAY     -0.01220512 0.04411997 [-0.08150817  0.05744370]
##      ratio      rhat       mcse
##  2.1818074 0.9997550 0.00023499
##  2.4572368 0.9991559 0.00043328
##  2.5613430 0.9994376 0.00041408
##  2.7598469 0.9992344 0.00048260
##  2.0032154 0.9998780 0.00041076
##  2.8279406 0.9991336 0.00043505
##  2.5725236 0.9995941 0.00039057
##  3.0152040 0.9994788 0.00032767
##  2.4777777 0.9991896 0.00043418
##  2.3137738 0.9993438 0.00042323
##  2.6268565 0.9996845 0.00037233
##  2.9598856 0.9998424 0.00049091
##  2.0728858 0.9998027 0.00038704
##  2.2142842 0.9997122 0.00047253
##  2.0993012 0.9995845 0.00046585
##  1.9617541 0.9993364 0.00030314
##  2.4610031 0.9992859 0.00052189
##  1.8691552 0.9992661 0.00020255
##  1.9000720 0.9997425 0.00048933
##  1.8565426 0.9994294 0.00048110
##  1.8927476 0.9992329 0.00024361
##  2.3675726 0.9992190 0.00037285
##  3.1252169 0.9993709 0.00048064
##  2.1241186 0.9994033 0.00046996
##  2.0478972 0.9992696 0.00040075
##  2.5648218 0.9993922 0.00044279
##  2.3418549 0.9994033 0.00043797
##  2.5010362 0.9997160 0.00042505
##  2.6030475 0.9991490 0.00048058
##  2.2089670 0.9991878 0.00026509
##  1.7579935 1.0003589 0.00023301
##  2.6319052 0.9995305 0.00045625
##  2.4505638 0.9993386 0.00047656
##  2.1380614 0.9991317 0.00044027
##  2.4383565 0.9991119 0.00042839
##  2.5688059 0.9992939 0.00032275
##  2.1443732 0.9993478 0.00046443
##  2.0884145 0.9992803 0.00045157
##  2.3561895 0.9992678 0.00046923
##  2.0744241 0.9998912 0.00041204
##  2.7251334 0.9990794 0.00045642
##  2.3392538 0.9992817 0.00041062
##  2.8293315 0.9990859 0.00043759
##  2.5669882 0.9991737 0.00043507
##  2.4365008 0.9992963 0.00047534
##  2.2426999 0.9992796 0.00043634
##  2.3222727 0.9991746 0.00044748
##  2.0689730 0.9996485 0.00070731
##  0.9547586 0.9993514 0.00041015
##  2.1275302 0.9995131 0.00046620
##  2.1861207 0.9992289 0.00052734
##  1.7805776 1.0000333 0.00048274
##  2.0041044 0.9992595 0.00042838
##  2.4199018 0.9990636 0.00045029
##  2.0091619 0.9997152 0.00043488
##  2.2952513 0.9996150 0.00048352
##  1.7779781 0.9993393 0.00032916
##  1.5125594 0.9996627 0.00048739
##  2.4542017 0.9995987 0.00025388
##  2.7278600 0.9992894 0.00043141
##  2.3980566 0.9991726 0.00048727
##  2.0140330 0.9997964 0.00027106
##  2.0041395 0.9999523 0.00040185
##  2.6746836 0.9993666 0.00042242
##  2.5977830 0.9997816 0.00047096
##  2.1528373 0.9997495 0.00049823
##  1.9778659 0.9996554 0.00043823
##  2.3924208 0.9992967 0.00031829
##  2.5213750 0.9992905 0.00043476
##  2.1384244 0.9993902 0.00045040
##  2.0926666 0.9992243 0.00031969
##  2.1690933 0.9992513 0.00043999
##  2.4973997 0.9992707 0.00047190
##  2.1825897 0.9993629 0.00037897
##  2.6408917 0.9991332 0.00042321
##  2.5648995 0.9990540 0.00045801
##  2.3518327 0.9998842 0.00044890
##  2.3058472 0.9994381 0.00044156
##  2.3262753 0.9996987 0.00042746
##  2.6144229 0.9994679 0.00051086
##  1.9555323 0.9995098 0.00035418
##  2.1448407 0.9995987 0.00051961
##  1.6115911 0.9996644 0.00031248
##  2.0181763 0.9995017 0.00030954
##  2.1628757 0.9994157 0.00034866
##  2.1867004 0.9994377 0.00047390
##  2.2753673 0.9992896 0.00046932
##  2.1039869 0.9999293 0.00037195
##  2.5783497 0.9993017 0.00060837
##  1.2186109 0.9998281 0.00035890
##  2.2584380 1.0000352 0.00039249
##  3.0448021 0.9990646 0.00042730
##  2.6142051 0.9993675 0.00040138
##  2.3823821 0.9996318 0.00045249
##  1.6799049 0.9995740 0.00029972
##  2.2245184 0.9991599 0.00046809
##  2.1622855 0.9999709 0.00037433
##  2.9136692 0.9991898 0.00058204
##  1.4528219 0.9995897 0.00043188
##  2.5348668 0.9991748 0.00039537
##  2.5502643 0.9993894 0.00045162
##  2.3717167 0.9994271 0.00047560
##  2.2310451 0.9992981 0.00036828
##  2.4360783 0.9995298 0.00042613
##  2.4954074 0.9994068 0.00044247
##  2.5647101 0.9991716 0.00038708
##  2.5346935 0.9993118 0.00040307
##  2.8556908 0.9995188 0.00054925
##  1.4944868 0.9998972 0.00034670
##  2.7751544 0.9996560 0.00052919
##  1.7442728 0.9995469 0.00030641
##  2.6854728 0.9992607 0.00039395
##  2.2669503 0.9992271 0.00027167
##  3.0883877 0.9992704 0.00044594
##  1.9841192 0.9993452 0.00027769
##  2.8450274 0.9993527 0.00044023
##  2.4471837 0.9995983 0.00036654
##  2.7880142 0.9990836 0.00045355
##  2.2254062 0.9992359 0.00044412
##  2.4806658 0.9994710 0.00039759
##  2.8174722 0.9994663 0.00051303
##  1.7680191 0.9992413 0.00031891
##  1.7420806 0.9996009 0.00042762
##  1.9908405 0.9994381 0.00036962
##  2.5375471 0.9992457 0.00043872
##  2.4020454 0.9992024 0.00049030
##  2.0647386 0.9990848 0.00033162
##  2.3286054 0.9992578 0.00049602
##  1.9451586 0.9997374 0.00048708
##  1.8688438 0.9997382 0.00050901
##  1.4878174 1.0001541 0.00040673
##  2.7271676 0.9991793 0.00044792
##  1.9296968 0.9992142 0.00048024
##  2.2343190 0.9994260 0.00031457
##  2.2757056 0.9995888 0.00042949
##  2.4810069 0.9993832 0.00040519
##  2.3932656 0.9998225 0.00046386
##  2.4506677 0.9993777 0.00044402
##  2.3702281 0.9995875 0.00046080
##  2.0693548 0.9996114 0.00059887
##  1.4330732 0.9997207 0.00042810
##  2.1773872 0.9993911 0.00055155
##  1.6377480 1.0007359 0.00015679
##  2.4051015 0.9993776 0.00041297
##  2.7772019 0.9993605 0.00042841
##  2.6766822 0.9990785 0.00042015
##  2.5206834 0.9992288 0.00044113
##  2.5572751 0.9992840 0.00046601
##  2.3560773 0.9992041 0.00041529
##  2.3240417 0.9995994 0.00050426
##  1.9320594 0.9994708 0.00043367
##  2.4414188 0.9991220 0.00038350
##  2.0795397 0.9995215 0.00039268
##  2.5321126 0.9991904 0.00045347
##  2.1695599 0.9994297 0.00031416
##  2.6771626 0.9995143 0.00046894
##  2.1350090 0.9999410 0.00040823
##  2.8324938 0.9993487 0.00032762
##  2.1491706 0.9994222 0.00041069
##  2.2236480 0.9995638 0.00044782
##  2.4663109 0.9992583 0.00035014
##  2.8063590 0.9991007 0.00046291
##  2.1988646 0.9998916 0.00045318
##  2.1137257 0.9992109 0.00042021
##  2.4538038 0.9991329 0.00048994
##  2.2288932 0.9993326 0.00048848
##  2.1253404 0.9992740 0.00045076
##  2.3517069 0.9996395 0.00037466
##  2.8913405 0.9992132 0.00053944
##  1.5848719 1.0000699 0.00034390
##  2.0985133 0.9995146 0.00047713
##  2.2767138 0.9991464 0.00038411
##  2.2836963 0.9997018 0.00049188
##  2.0111386 0.9994816 0.00045384
##  2.2422181 0.9993922 0.00046000
##  2.1348027 0.9992397 0.00045710
##  2.0762325 0.9991623 0.00032940
##  2.3002480 0.9995515 0.00047104
##  2.1353847 0.9993037 0.00041447
##  2.7207297 0.9994036 0.00038115
##  2.7023797 0.9994827 0.00044511
##  2.4340113 0.9991849 0.00046153
##  2.1734229 0.9994292 0.00036030
##  2.5628822 0.9995523 0.00034926
##  2.1897336 0.9997436 0.00043356
##  2.4784786 0.9997429 0.00048832
##  1.8770289 0.9993866 0.00018281
##  2.2427244 0.9997993 0.00044089
##  2.4311238 0.9991031 0.00051388
##  1.8477531 0.9997561 0.00017382
##  2.3604016 0.9993120 0.00026014
##  1.5377350 0.9991885 0.00040513
##  3.0516578 0.9995917 0.00046270
##  2.2696808 0.9992260 0.00041265
##  2.5149837 0.9992074 0.00042878
##  2.4264453 0.9995202 0.00042836
##  2.5205045 0.9994955 0.00058501
##  1.4222643 0.9995528 0.00042694
##  2.4804106 0.9997478 0.00050936
##  1.9280976 0.9993411 0.00041271
##  2.9028913 0.9994906 0.00048173
##  2.2850763 0.9996373 0.00050175
##  1.9694895 0.9992923 0.00033154
##  2.1655440 0.9994712 0.00031338
##  2.2260291 0.9994371 0.00035390
##  3.0440127 0.9991328 0.00037455
##  3.0068171 0.9992952 0.00036526
##  2.5258742 0.9993333 0.00044045
##  2.4618618 0.9993255 0.00025851
##  2.4347206 0.9992264 0.00025768
##  2.0083769 0.9994405 0.00048433
##  2.1223732 0.9996383 0.00061962
##  1.2248752 0.9993745 0.00047276
##  2.4749185 1.0001065 0.00040830
##  2.2037084 0.9997567 0.00049703
##  1.8256276 0.9994930 0.00012853
##  2.3012175 0.9992703 0.00035297
##  2.2867794 0.9993554 0.00042524
##  2.7482692 0.9993127 0.00043276
##  2.6918304 0.9994877 0.00044363
##  2.4821675 0.9994535 0.00053263
##  1.7166565 0.9994000 0.00022132
##  2.5756159 0.9991691 0.00045274
##  2.2752323 0.9992232 0.00039621
##  2.5453827 0.9994262 0.00047584
##  2.1045634 0.9997382 0.00046616
##  2.4039531 0.9994952 0.00045064
##  2.3194735 0.9991392 0.00027220
##  2.1342514 0.9995275 0.00042044
##  2.4473883 0.9993521 0.00046203
##  2.2416105 0.9991412 0.00089507
## 
## ## Random effect (Intercept: word.AIN)
## 
##                estimate  std.error                  HDI(89%)    ratio
##  word.AIN.T -0.00825166 0.04150287 [-0.07835311  0.05806501] 2.640339
##       rhat       mcse
##  0.9994529 0.00043525
## 
## ## Random effect (Intercept: word.ASIAN)
## 
##                  estimate  std.error                  HDI(89%)    ratio
##  word.ASIAN.S -0.00401908 0.04250149 [-0.07815976  0.05765512] 1.982073
##       rhat       mcse
##  0.9995977 0.00047366
## 
## ## Random effect (Intercept: word.BABY)
## 
##                 estimate  std.error                  HDI(89%)    ratio
##  word.BABY.S -0.01659658 0.04126606 [-0.08328705  0.04952631] 2.434151
##       rhat       mcse
##  0.9992437 0.00048279
## 
## ## Random effect (Intercept: word.BIRTHDAY)
## 
##                    estimate  std.error                  HDI(89%)    ratio
##  word.BIRTHDAY.S 0.01486289 0.04141658 [-0.04645772  0.08663963] 2.000037
##       rhat      mcse
##  0.9993835 0.0004102
## 
## ## Random effect (Intercept: word.GATE)
## 
##                estimate  std.error                  HDI(89%)    ratio
##  word.GATE.S 0.01564178 0.04166492 [-0.05491001  0.08007111] 1.972399
##       rhat       mcse
##  0.9995484 0.00047734
## 
## ## Random effect (Intercept: word.INVESTIGATOR)
## 
##                         estimate  std.error                  HDI(89%)
##  word.INVESTIGATOR.S -0.00924654 0.04254645 [-0.07694268  0.05935461]
##     ratio     rhat       mcse
##  3.214484 0.999402 0.00043566
## 
## ## Random effect (Intercept: word.M)
## 
##                estimate  std.error                  HDI(89%)    ratio
##  word.M.KAY -0.01348873 0.04328078 [-0.08190046  0.05375403] 2.162435
##       rhat       mcse
##  0.9991532 0.00048318
## 
## ## Random effect (Intercept: word.NAME)
## 
##                estimate  std.error                  HDI(89%)    ratio
##  word.NAME.S -0.0037259 0.04219941 [-0.07340668  0.06756139] 2.369684
##      rhat       mcse
##  0.999458 0.00046308
## 
## ## Random effect (Intercept: word.PAPER)
## 
##                 estimate  std.error                  HDI(89%)    ratio
##  word.PAPER.S 0.00139462 0.04039734 [-0.06512917  0.06732175] 2.098143
##       rhat       mcse
##  0.9992044 0.00046555
## 
## ## Random effect (Intercept: word.THEY)
## 
##                  estimate  std.error                  HDI(89%)    ratio
##  word.THEY.D  -0.00510002 0.04423727 [-0.07465584  0.05897770] 2.242487
##  word.THEY.VE -0.02464330 0.04143074 [-0.09254060  0.03624197] 1.994954
##       rhat       mcse
##  0.9992971 0.00048743
##  0.9997810 0.00042571
## 
## ## Random effect (Intercept: word.TODAY)
## 
##                 estimate  std.error                  HDI(89%)    ratio
##  word.TODAY.S 0.00656973 0.04261816 [-0.06376907  0.07449111] 2.561907
##       rhat      mcse
##  0.9992971 0.0004628</code></pre>
<p>The tidy_stan summary with HPDI 89% credible intervals and the ordinary summary with 95% symmetrical credible intervals agree with each other, which is good – shows the effects aren’t flimsy.</p>
<p>Plot the posteriors of the fixed effects with 89% HPDI credible intervals:</p>
<pre class="r"><code>posterior2 &lt;- as.matrix(model2)

#dimnames(posterior2)

mcmc_areas(posterior2,
           pars = c(&quot;b_Intercept&quot;, 
                    &quot;b_Log_dur_z&quot;,
                    &quot;b_agechild&quot;, 
                    &quot;b_SexF&quot;, 
                    &quot;b_agechild:SexF&quot;),
           prob=0.89)</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<div id="plotting-bayesian-output-1" class="section level3">
<h3>Plotting Bayesian output</h3>
<p>Plot marginal_effects:</p>
<pre class="r"><code>marg2 &lt;- marginal_effects(model2, ask = FALSE)

p &lt;- plot(marg2, plot = FALSE)[[4]]

p  + theme_minimal() + labs(y = &quot;Normalized Trajectory Length&quot;, x = &quot;Age&quot;)</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Save marg2 to a new df:</p>
<pre class="r"><code>face_traj &lt;- as.data.frame(marg2[4])</code></pre>
<p>Check structure</p>
<pre class="r"><code>str(face_traj)</code></pre>
<pre><code>## &#39;data.frame&#39;:    4 obs. of  13 variables:
##  $ age.Sex.age        : Factor w/ 2 levels &quot;adolescent&quot;,&quot;child&quot;: 1 1 2 2
##  $ age.Sex.Sex        : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 1 2 1 2
##  $ age.Sex.norm_TL    : num  0.37 0.37 0.37 0.37
##  $ age.Sex.Log_dur_z  : num  -0.00818 -0.00818 -0.00818 -0.00818
##  $ age.Sex.participant: logi  NA NA NA NA
##  $ age.Sex.word       : logi  NA NA NA NA
##  $ age.Sex.cond__     : Factor w/ 1 level &quot;1&quot;: 1 1 1 1
##  $ age.Sex.effect1__  : Factor w/ 2 levels &quot;adolescent&quot;,&quot;child&quot;: 1 1 2 2
##  $ age.Sex.effect2__  : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 1 2 1 2
##  $ age.Sex.estimate__ : num  0.325 0.399 0.398 0.366
##  $ age.Sex.se__       : num  0.0179 0.0175 0.0185 0.0191
##  $ age.Sex.lower__    : num  0.289 0.364 0.36 0.328
##  $ age.Sex.upper__    : num  0.361 0.437 0.436 0.405</code></pre>
<p>Define columns to keep:</p>
<pre class="r"><code>keep1 &lt;- c(&quot;age.Sex.age&quot;, &quot;age.Sex.Sex&quot;, &quot;age.Sex.estimate__&quot;, &quot;age.Sex.se__&quot;, &quot;age.Sex.lower__&quot;, &quot;age.Sex.upper__&quot;)</code></pre>
<p>Perform “select”:</p>
<pre class="r"><code>face_traj2 &lt;- face_traj %&gt;% select(keep1)</code></pre>
<p>Rename columns:</p>
<pre class="r"><code>colnames(face_traj2) &lt;- c(&quot;age&quot;,&quot;Sex&quot;,&quot;norm_TL&quot;, &quot;SE&quot;,&quot;lower&quot;, &quot;upper&quot;)</code></pre>
<p>Plot it:</p>
<pre class="r"><code>f &lt;- ggplot(face, aes(x = age, y = norm_TL, fill = Sex)) + 
  geom_violin(color = NA) + 
  geom_boxplot(width = 0.3, position = position_dodge(0.9), color = &quot;#999999&quot;) + 
  theme_minimal() + 
  scale_fill_manual(values = binary) + 
   theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.title = element_text(size = rel(1.5)),
        legend.text = element_text(size = rel(1.5)),
        axis.title = element_text(size = rel(2)), 
        axis.text = element_text(size = rel(1.5)), 
        plot.title = element_text(size = rel(2))) +
  labs(x = &quot;Age&quot;, y = &quot;Trajectory Length&quot;, title = &quot;FACE: Trajectory Length&quot;) +
  ylim(0, 2)
f</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-41-1.png" width="768" /></p>
<pre class="r"><code>f2 &lt;- f + geom_point(data = face_traj2, aes(x=age, y=norm_TL), size = 4, position = pd) +
  geom_errorbar(data = face_traj2, aes(ymin=lower, ymax=upper), width=.1, size = 1, position = pd)

f2</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-42-1.png" width="768" /></p>
</div>
</div>
</div>
<div id="price" class="section level1">
<h1>PRICE</h1>
<p>Create the subset:</p>
<pre class="r"><code>price &lt;- diphs3 %&gt;% filter(sound_label == &quot;price&quot;)</code></pre>
<div id="price-f2-at-20" class="section level2">
<h2>PRICE F2 at 20%</h2>
<p>Figure out what priors we need:</p>
<pre class="r"><code>get_prior(normF2_20 ~ Log_dur_z + age*Sex + (1|participant) + (1|word), data = price)</code></pre>
<pre><code>##                  prior     class          coef       group resp dpar nlpar
## 1                              b                                          
## 2                              b      agechild                            
## 3                              b agechild:SexF                            
## 4                              b     Log_dur_z                            
## 5                              b          SexF                            
## 6  student_t(3, 1, 10) Intercept                                          
## 7  student_t(3, 0, 10)        sd                                          
## 8                             sd               participant                
## 9                             sd     Intercept participant                
## 10                            sd                      word                
## 11                            sd     Intercept        word                
## 12 student_t(3, 0, 10)     sigma                                          
##    bound
## 1       
## 2       
## 3       
## 4       
## 5       
## 6       
## 7       
## 8       
## 9       
## 10      
## 11      
## 12</code></pre>
<p>Check distribution of the dependent variable conditional on the independent variables:</p>
<pre class="r"><code>ggplot(price, aes(x=age, y = normF2_20, fill = Sex)) + geom_violin()</code></pre>
<p><img src="icphs_regression_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>Set some priors:</p>
<pre class="r"><code>priors3 &lt;- c(set_prior(&quot;normal(1, 1)&quot;, class = &quot;Intercept&quot;), #
              set_prior(&quot;normal(0, 2)&quot;, class = &quot;b&quot;, coef = &quot;Log_dur_z&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;b&quot;, coef = &quot;agechild&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;b&quot;, coef = &quot;SexF&quot;),
              set_prior(&quot;normal(0, 1.5)&quot;, class = &quot;b&quot;, coef = &quot;agechild:SexF&quot;), #2*age effect to allow for complete reversal
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group=&quot;participant&quot;),
              set_prior(&quot;normal(0, 0.75)&quot;, class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group=&quot;word&quot;)
              )</code></pre>
<p>Build the model:</p>
<pre class="r"><code>model3 &lt;- brm(normF2_20 ~ Log_dur_z + age*Sex + (1|participant) + (1|word), data = price, prior = priors3)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;f6033d44a78cd545630f3fd5bac49010&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.001313 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 13.13 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 80.3328 seconds (Warm-up)
## Chain 1:                36.6762 seconds (Sampling)
## Chain 1:                117.009 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;f6033d44a78cd545630f3fd5bac49010&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.000548 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 5.48 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 78.6183 seconds (Warm-up)
## Chain 2:                36.0864 seconds (Sampling)
## Chain 2:                114.705 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;f6033d44a78cd545630f3fd5bac49010&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0.000656 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 6.56 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 74.6868 seconds (Warm-up)
## Chain 3:                29.6305 seconds (Sampling)
## Chain 3:                104.317 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;f6033d44a78cd545630f3fd5bac49010&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0.000293 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 2.93 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 82.2361 seconds (Warm-up)
## Chain 4:                32.6454 seconds (Sampling)
## Chain 4:                114.882 seconds (Total)
## Chain 4:</code></pre>
<p>Check the summary:</p>
<pre class="r"><code>summary(model3)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: normF2_20 ~ Log_dur_z + age * Sex + (1 | participant) + (1 | word) 
##    Data: price (Number of observations: 3104) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~participant (Number of levels: 28) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.07      0.01     0.06     0.10       1252 1.00
## 
## ~word (Number of levels: 237) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.06      0.01     0.05     0.07        956 1.00
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept         1.04      0.03     0.98     1.10        905 1.00
## Log_dur_z        -0.03      0.00    -0.03    -0.02       6293 1.00
## agechild         -0.02      0.04    -0.10     0.06        807 1.00
## SexF             -0.11      0.04    -0.19    -0.04        932 1.00
## agechild:SexF     0.14      0.06     0.02     0.25        820 1.01
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     0.10      0.00     0.10     0.11       6551 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>And the tidystan summary:wr</p>
<div id="plotting-bayesian-output-2" class="section level3">
<h3>Plotting Bayesian output</h3>
</div>
</div>
<div id="price-trajectory-length" class="section level2">
<h2>PRICE Trajectory Length</h2>
<div id="plotting-bayesian-output-3" class="section level3">
<h3>Plotting Bayesian output</h3>
</div>
</div>
</div>
<div id="goat" class="section level1">
<h1>GOAT</h1>
<div id="goat-f2-at-20" class="section level2">
<h2>GOAT F2 at 20%</h2>
<div id="plotting-bayesian-output-4" class="section level3">
<h3>Plotting Bayesian output</h3>
</div>
</div>
<div id="goat-trajectory-length" class="section level2">
<h2>GOAT Trajectory Length</h2>
<div id="plotting-bayesian-output-5" class="section level3">
<h3>Plotting Bayesian output</h3>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
